"use strict";(self.webpackChunkgoose=self.webpackChunkgoose||[]).push([[583],{9314:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2025/04/11/finetuning-toolshim","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/04/11/finetuning-toolshim","source":"@site/blog/2025-04-11-finetuning-toolshim/index.md","title":"Finetuning Toolshim Models for Tool Calling","description":"Addressing performance limitations in models without native tool calling support","date":"2025-04-11T00:00:00.000Z","tags":[],"readingTime":5.46,"hasTruncateMarker":true,"authors":[{"name":"Alice Hau","title":"Machine Learning Engineer","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/alice"},"socials":{"linkedin":"https://www.linkedin.com/in/alice-hau/","github":"https://github.com/alicehau"},"imageURL":"https://avatars.githubusercontent.com/u/110418948?v=4","key":"alice"},{"name":"Michael Neale","title":"Principal Engineer","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/mic"},"socials":{"github":"https://github.com/michaelneale"},"imageURL":"https://avatars.githubusercontent.com/u/14976?v=4","key":"mic"}],"frontMatter":{"title":"Finetuning Toolshim Models for Tool Calling","description":"Addressing performance limitations in models without native tool calling support","authors":["alice","mic"]},"unlisted":false,"nextItem":{"title":"How to Vibe Code Responsibly (with Goose)","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/08/vibe-code-responsibly"}},"content":"![blog cover](toolshim-header.png)\\n\\nOur recently published [Goose benchmark](https://block.github.io/goose/blog/2025/03/31/goose-benchmark) revealed significant performance limitations in models where tool calling is not straightforwardly supported (e.g., Gemma3, Deepseek-r1, phi4). These models often fail to invoke tools at appropriate times or produce malformed or inconsistently formatted tool calls. With the most recent releases of Llama4 and Deepseek v3 (0324), we are again observing challenges with effective tool calling performance, even on these flagship openweight models.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why tool calling is important\\n\\nTool calling is a critical capability for agents like goose. It allows models to go beyond text and image generation and take concrete actions, such as executing code, querying databases, searching the web, or interacting with design tools like Figma. Equipping agents with a broad set of tools empowers them to discover and interface with external systems, much like a human would. While this might be overkill for narrow, more deterministic applications of LLMs, it is essential for general-purpose agents like goose. Without reliable tool calling, we limit what models can do to help us automate, remove toil and navigate complex systems. Pure generation\u2013of text, images, speech, and video\u2013is just the first step on the path to more powerful agentic capabilities. There is so much more that models can do if we give them the legs to run.\\n\\n## Background: using a local model as a \\"toolshim\\"\\n\\nThe goal is to allow goose to work with the widest variety of models possible. A \\"toolshim\\" in this case is a thin layer which sits between the main model doing the agent work, and the tools that can perform actual actions (making the agent take action, vs being a chatbot). Previously we have been trying this approach with open models including in this [past benchmark](https://block.github.io/goose/blog/2025/03/31/goose-benchmark) post. A toolshim, if it can work, unlocks both powerful cutting edge models (open weight and closed) which while may perform well on various benchmarks, fall well short when tool calling for agents is required (or perhaps don\'t, by design, support tool calling at all, such as the case with some reasoning models).\\n\\n## Proposal: Fine-tune a lightweight toolshim model (up to 12b)\\n\\nDevelop a dedicated toolshim model that translates open-source model outputs into well-structured tool calls, acting as a reliable post-processor to standardize across model families trained that currently exhibit inconsistent and unreliable tool call generation behavior. We do not use tool calling apis even if available, but provide tool context in the system prompts.\\n\\nWe already experimented with this in the [benchmarking effort](https://block.github.io/goose/blog/2025/03/31/goose-benchmark), finding that phi4 (14b) and gemma3 (27b) achieved close performance to llama3.3 (70b) when used with a generic local model (mistral-nemo) as the shim. This shows potential for furthering their performance with more focused attention on improving the shim\'s performance.\\n\\nToolshim System Sketch:\\n\\n![Toolshim System Sketch](./sketch.png)\\n\\n## Key Observations on Current Challenges with Tool Call Generation\\n\\n1. **Model training templates are inconsistent**  \\n   For example, [Qwen models use](https://qwen.readthedocs.io/en/latest/framework/function_call.html) [Hermes-style tool formats](https://github.com/NousResearch/Hermes-Function-Calling), while Openhands generates Markdown despite explicit JSON instructions\u2014suggesting training data shape can have an underestimated impact on reliable tool call generation  \\n\\n2. **Current workarounds aren\'t enough**  \\n   [Model providers may implement approaches like guided decoding](https://docs.vllm.ai/en/latest/features/tool_calling.html) to guarantee validly-parsable function calls, but these may not produce high-quality outputs if the model wasn\'t trained on schemas matching what users provide in context. The widespread challenges with tool use with Llama4 may be indicative of the challenges providers have in effectively serving new models to make full use of their capabilities  \\n\\n3. **Hosting providers vary wildly in how well they work with tool calls**  \\n   Hosting providers helpfully provide chat templates or similar which can, in many cases, prompt some of the larger models to reply correctly formatted tool calls, and thus can support openai-like apis where tools are provided, but in practice these can fall short after one shot, or vary a lot between providers (an issue exacerbated if using model routers such as openrouter or huggingface hosted inference)\\n\\n### Some examples of model-specific quirks wrt tool calling:\\n\\n**Openhands**: Despite instructions to generate JSON-formatted tool calls, still generates markdown (likely due to shape of their training data) \\n\\n![Openhands example](./openhands.png)\\n\\n**Llama4 Maverick**: Generates malformed tool calls, but performs somewhat better when specifically prompted to generate tool calls as JSON\\n\\nWith \\"tool calls\\" on OpenRouter:  \\n![OpenRouter tool calls example](./openrouter_toolcalls.png)\\n\\nLlama4 Maverick when instead just prompted to generate tool calls in JSON:  \\n![Llama4 example](./llama4.png)\\n\\n**Gemma3**: A DeepMind engineer [suggested providing a function calling template in-context in Python format](https://www.philschmid.de/gemma-function-calling)  \\nThe 12B model also outputs valid JSON tool calls reasonably well:  \\n![Gemma3 example](./gemma3.png)\\n\\n**Functionary models**: [Ollama couldn\'t support the tool calling capabilities](https://github.com/MeetKai/functionary/issues/302#issuecomment-2650187280) because these models were trained with prompt templates in a TypeScript schema incompatible with Ollama\'s supported JSON schema\\n\\n## Experimentation Approach\\n\\n### Data Collection\\n\\n* Extract user messages from historical Goose sessions, and for messages followed by tool calls from Anthropic/OpenAI (all tool calls up to today):  \\n  * **Regenerate tool calls with open models:** Regenerate the tool calls with the most capable open models that have supported tool calling capabilities (e.g., QwQ, Qwen, deepseek chat v3)  \\n  * **Generate json/markdown-formatted tool calls to parse:** Instruct the most capable open models (e.g., DeepSeek-r1, Llama4, Gemma3), that don\'t necessarily have strong tool calling to output tool calls in the correct schema (JSON/markdown). Parse the output into the appropriate tool calls.  \\n  * **Discard any malformed tool calls, tool calls that fail to properly execute, or tool calls that meet other rejection criteria**  \\n* Generate a few thousand examples with this approach\\n\\n### Modeling \\n\\nFine tune small models like mistral-nemo (14b), gemma 4-12b, qwen2.5-coder 7-14b. \\n\\n### Evaluations\\n\\nTest with Goosebench evals run in the benchmarking blogpost. We can directly compare performance of models with and without the finetuned toolshim models supporting them.\\n\\n## Future approaches\\n\\nOn top of local models, we would like to consider parsers, parser combinators, context-free grammars and more (even very large ones) which are constructed based on 1000s of examples of tool results. Even if large, these can operate at every low latencies extracting parameters for suggested tool calls. There are likely other structured text extraction techniques to be explored to assist with discovery and extraction of tool calls from rich responses from powerful general models.\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Finetuning Toolshim Models for Tool Calling\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/04/11/finetuning-toolshim\\" />\\n  <meta property=\\"og:description\\" content=\\"Addressing performance limitations in models without native tool calling support\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/toolshim-header-42611f614e7722f90cf83991debe3046.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Finetuning Toolshim Models for Tool Calling\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Addressing performance limitations in models without native tool calling support\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/toolshim-header-42611f614e7722f90cf83991debe3046.png\\" />\\n</head>"},{"id":"/2025/04/08/vibe-code-responsibly","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/04/08/vibe-code-responsibly","source":"@site/blog/2025-04-08-vibe-code-responsibly/index.md","title":"How to Vibe Code Responsibly (with Goose)","description":"Vibe coding feels magical until it isn\'t. Learn how to flow with Goose while protecting your code, your team, and your future self.","date":"2025-04-08T00:00:00.000Z","tags":[],"readingTime":5.64,"hasTruncateMarker":true,"authors":[{"name":"Rizel Scarlett","title":"Staff Developer Advocate","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/rizel"},"socials":{"x":"https://x.com/blackgirlbytes","github":"https://github.com/blackgirlbytes"},"imageURL":"https://avatars.githubusercontent.com/u/22990146?v=4","key":"rizel"}],"frontMatter":{"title":"How to Vibe Code Responsibly (with Goose)","description":"Vibe coding feels magical until it isn\'t. Learn how to flow with Goose while protecting your code, your team, and your future self.","authors":["rizel"]},"unlisted":false,"prevItem":{"title":"Finetuning Toolshim Models for Tool Calling","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/11/finetuning-toolshim"},"nextItem":{"title":"Top 5 MCP Servers I Use as a Developer with Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/01/top-5-mcp-servers"}},"content":"![blog cover](responsible-vibe-code.png)\\n\\nOn Feb 2, 2025, Andrej Karpathy coined the phrase \\"[vibe coding](https://x.com/karpathy/status/1886192184808149383)\\". Vibe coding represents a new approach to coding where developers ask an AI agent to build something, and they go with the flow. \\n\\nThe [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) makes this practice possible. Before MCP, developers copied and pasted context between applications. This workflow fell short of the promised AI agent automation that everyone claimed. Today, AI agents can work autonomously using MCP and integrate with any application, from GitHub to Cloudflare, YouTube, and Figma.\\n\\nThis shift democratizes coding. For example, it\'s empowered:\\n\\n* Web developers to create video games with Unity  \\n* Designers and product managers to prototype full-stack applications  \\n* Business owners to transform their visions into functional products\\n\\nIt\'s a freeing experience. But too often, we\'re [Icarus](https://www.britannica.com/topic/Icarus-Greek-mythology) with the keyboard, vibe coding too close to the sun.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Dark Side of Vibe Coding\\n\\nThis creative freedom comes with significant risks. Many developers have encountered serious issues while vibe coding:\\n\\n* Committing code with security vulnerabilities  \\n* Introducing difficult-to-fix bugs on top of \\"spaghetti\\" code  \\n* Losing weeks or months of work due to lack of version control  \\n* Accidentally exposing sensitive information like environment variables and API keys in production\\n\\n<blockquote className=\\"twitter-tweet\\" data-dnt=\\"true\\" align=\\"center\\"><p lang=\\"en\\" dir=\\"ltr\\">Today was the worst day ever\u2639\ufe0f<br />The project I had been working on for the last two weeks got corrupted, and everything was lost. Just like that, my SaaS was gone. Two weeks of hard work, completely ruined.<br />But!!!<br />I started from scratch and have already completed 50% of the work\u2026</p>&mdash; CC Anuj (@vid_anuj) <a href=\\"https://twitter.com/vid_anuj/status/1902379748501880934?ref_src=twsrc%5Etfw\\">March 19, 2025</a></blockquote>\\n<script async src=\\"https://platform.twitter.com/widgets.js\\" charSet=\\"utf-8\\"><\/script>\\n\\n\\n## A Better Way to Vibe Code with Goose\\n\\n[Goose](https://block.github.io/goose) is an open source AI agent local to your machine with built-in features for safe vibe coding.  \\n\\n:::note\\nMost folks define \\"vibe coding\\" as purely chaotic development with no rules. I\'m redefining it as flowing with AI while protecting your project, team, and future self.\\n:::\\n\\n### 1. Use `.gooseignore` to Protect Sensitive Files\\n\\nGoose supports [`.gooseignore`](https://block.github.io/goose/docs/guides/using-gooseignore) files. The concept is similar to `.gitignore` files for your AI agent. It defines which files and folders Goose should *not* read, modify, or interact with.\\n\\nUse this when you want to prevent:\\n\\n* Accidental changes to environment variables  \\n* Modifications to sensitive configs  \\n* Changes to test fixtures or snapshots  \\n* Edits to infrastructure and deployment configs  \\n* Changes to code examples or documentation  \\n* Shell commands running in places they shouldn\'t\\n\\n### 2. Create a plan\\n\\nGoose\'s [`/plan`](https://block.github.io/goose/docs/guides/goose-cli-commands#examples) command helps you align with your agent before any code is touched, giving you a clear understanding of what it intends to do and how it will do it.\\n\\nThis is especially useful for tasks that span multiple files, involve side effects, or could impact critical areas of your codebase. No more guesswork\u2014just a structured breakdown you can review and approve.\\n\\n### 3. Choose the Right Mode for the Job\\n\\nWhile letting your AI agent take the lead is fun, not every moment calls for full autonomy. Sometimes, you need to pause, review, or plan before any code changes. Goose offers several [modes](https://block.github.io/goose/docs/guides/goose-permissions) that help you stay in control without breaking your momentum. Here\'s how to use them intentionally during your sessions:\\n\\n* **Chat Mode**  \\n  Goose will only respond with text so that you can brainstorm together.\\n\\n* **Approval Mode**  \\n  Before Goose executes an action, it asks for your approval. This is helpful when you want to keep building fast but still want to know what\'s about to happen before it does.\\n\\n* **Smart Approval**  \\n  In this mode, Goose requests your approval for risky actions. This mode is helpful for prototyping quickly while keeping guardrails in place.\\n\\n* **Autonomous Mode**  \\n  In this mode, Goose moves forward without asking for approval. Using this mode is best if you feel confident in the direction and have safety nets in place.\\n\\n### 4. Use Version Control Religiously\\n\\nThere are moments when AI agents change too many files and lines that the Control + Z can\'t fix. It\'s best to commit to every change that you or Goose make to get recovery points, clear diffs, and the ability to revert quickly.  \\n\\n### 5. Ask Questions and Think Critically\\n\\nEven if you\'re vibe coding, don\'t turn off your brain.\\n\\nAsk Goose:\\n\\n* Why did you make this change?  \\n* Is this secure?  \\n* How are we handling secrets?  \\n* Is this the best way to structure the database?\\n\\nBy pushing your agent to explain itself, you\'ll build a better product and learn more along the way.\\n\\n### 6. Define .goosehints for Better Context\\n\\nThe [.goosehints](https://block.github.io/goose/docs/guides/using-goosehints) file gives Goose additional context about your project\'s coding standards, architectural preferences, and security practices.  \\n\\nHere are a few examples:\\n\\n* \\"Never expose API keys.\\"  \\n* \\"Use prepared statements for database queries.\\"  \\n* \\"Avoid using eval or unsafe dynamic code.\\"\\n\\n### 7. Integrate Goose into Your CI/CD\\n\\nBefore issues hit production, add [Goose to your CI/CD pipeline](https://block.github.io/goose/docs/tutorials/cicd) to:\\n- Automate code reviews  \\n- Validate documentation  \\n- Run security checks  \\n\\n### 8. Use an Allowlist to Block Unsafe MCP Servers\\n\\nSome MCP servers can introduce security risks, especially if compromised.\\n\\nUse the Goose [allowlist](https://github.com/block/goose/blob/main/crates/goose-server/ALLOWLIST.md) feature to prevent Goose from calling unsafe or untrusted tools.\\n\\nHere\'s how the team at Block is thinking about [securing the MCP](https://block.github.io/goose/blog/2025/03/31/securing-mcp).\\n\\n### 9. Pick a High-Performing LLM\\n\\nNot all LLMs are built the same. Goose plays best with:\\n\\n* Claude Sonnet 3.5  \\n* GPT-4o  \\n\\nLower-performing models might work, but they\'re more likely to hallucinate or misunderstand your goals. Read more about how [different LLM\'s perform with Goose](https://block.github.io/goose/blog/2025/03/31/goose-benchmark/).  \\n\\n## Watch Vibe Coding in Action  \\nHere\u2019s how folks vibe code with Goose:  \\n\\n<iframe width=\\"560\\" height=\\"315\\" src=\\"https://www.youtube.com/embed/xZo3aA-vFi4?si=14bVczrCUwdKBZyg\\" title=\\"The Great Great Off\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n## Final Thoughts\\n\\nVibe coding isn\'t inherently wrong. It\'s marks a new chapter in how we build, and it opens the door for everyone. But experienced developers have a responsibility to define what smart, safe vibe coding looks like. Goose gives us the tools to set that standard, so the whole community can code creatively without sacrificing quality.\\n\\nDownload [Goose](https://block.github.io/goose/docs/getting-started/installation/), and start vibe coding with intention today!\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"How to Vibe Code Responsibly (with Goose)\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/04/08/vibe-code-responsibly\\" />\\n  <meta property=\\"og:description\\" content=\\"Vibe coding feels magical until it isn\'t. Learn how to flow with Goose while protecting your code, your team, and your future self.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/responsible-vibe-code-a77f5e24a879edda943cc76f1fc0bd2a.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"How to Vibe Code Responsibly (with Goose)\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Vibe coding feels magical until it isn\'t. Learn how to flow with Goose while protecting your code, your team, and your future self.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/responsible-vibe-code-a77f5e24a879edda943cc76f1fc0bd2a.png\\" />\\n</head>"},{"id":"/2025/04/01/top-5-mcp-servers","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/04/01/top-5-mcp-servers","source":"@site/blog/2025-04-01-top-5-mcp-servers/index.md","title":"Top 5 MCP Servers I Use as a Developer with Goose","description":"These 5 MCP servers help me automate my workflow and make me a better developer.","date":"2025-04-02T00:00:00.000Z","tags":[],"readingTime":5.105,"hasTruncateMarker":true,"authors":[{"name":"Adewale Abati","title":"Staff Developer Advocate","url":"https://adewaleabati.com","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/adewale"},"socials":{"x":"https://x.com/ace_kyd","github":"https://github.com/acekyd"},"imageURL":"https://avatars.githubusercontent.com/u/4003538?v=4","key":"adewale"}],"frontMatter":{"title":"Top 5 MCP Servers I Use as a Developer with Goose","description":"These 5 MCP servers help me automate my workflow and make me a better developer.","date":"2025-04-02T00:00:00.000Z","authors":["adewale"]},"unlisted":false,"prevItem":{"title":"How to Vibe Code Responsibly (with Goose)","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/08/vibe-code-responsibly"},"nextItem":{"title":"MCP Explained for Non-Developers","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/01/mcp-nondevs"}},"content":"![blog cover](mcp-servers-cover.png)\\n\\nAs a developer, finding the right tools that seamlessly work together can feel like discovering a superpower. And when you have a working process, it can sometimes be difficult to try out new tools.\\n\\nWith the introduction of MCPs, AI agents like Goose are able to plug in to my existing tools, and the only thing that changes with my workflow is that much welcomed automation that comes with it. I still do the same things I do, but backed by AI, I can now do them faster and with more confidence.\\n\\nToday, I\'m excited to share not just my favorite MCP servers, but the ones I actually use almost everyday with real applications that you can probably relate to as well.\\n\\n\x3c!--truncate--\x3e\\n\\n:::tip\\nYou can ask Goose what you can do with an extension to get a list of all the features and example use cases you can try out. \\n:::\\n\\n## GitHub MCP Server: Everything GitHub\\n\\nThe [GitHub MCP Server](/docs/tutorials/github-mcp) comes with quite a lot of functionality. It can help you create issues, pull requests, repositories, and branches. My most frequent use case for the GitHub MCP is reviewing and understanding pull requests.\\n\\nFor cases when it\'s a large pull request, or I don\'t understand what is going on, I can pass the PR to Goose, giving it the right context to make me understand and then act on the pull request. I\'m even able to create a documentation update or changelog update from the file changes in the PR. This is definitely one of my favorite things. \\n\\nE.g \\n\\n```\\nHey Goose, this pull request https://github.com/block/goose/pull/1949, has a lot of changes. Can you summarize into a changelog for me?\\n```\\n\\n## Knowledge Graph Memory: Context on Steroids\\n\\nThe [Knowledge Graph Memory](/docs/tutorials/knowledge-graph-mcp) extension is like giving Goose a photographic memory of your project or data. Like the name implies, it creates a graph of any information fed into it, connecting the dots between different pieces of information or as I like to use it for - documentation. \\n\\nIf I\'m working on a specific project or library and I don\'t want any hallucinations, I am able to feed Goose with the right context and it will be able to answer questions about the project or library with the right context.\\n\\nThis could be documentation of the project I\'m currently working on, or even documentation of a library I\'m using.\\n\\nE.g\\n\\n```\\nI\'m currently in a project called Goose, read through the documentation in `documentation/docs/` folder and store key information in the knowledge graph. Use it for reference anytime I ask you about Goose.\\n```\\n\\n## Fetch Extension: The Web in our Hands\\n\\nI had a slightly hard time deciding between the [Tavily Web Search Extension](/docs/tutorials/tavily-mcp) and The [Fetch Extension](/docs/tutorials/fetch-mcp) because while I do use them both to access the web, the Fetch extension works more like default for me. With the example above using the Knowledge graph, I\'m able to get information from the internet to give Goose additional context to work with. \\n\\n:::note\\nThe Tavily Web Search Extension has deep research capabilities and is great for finding specific information, while the Fetch Extension is more about general web access and data retrieval.\\n:::\\n\\n## Memory Extension: My Habits and Preferences\\n\\nI use the [Memory Extension](/docs/tutorials/memory-mcp) to remind Goose about my general preferences as I work - to default to JavaScript or Node when trying out new prototypes, if I prefer one naming convention or the other - maybe even how I like my coffee :D.\\n\\nThis works differently from the Knowledge Graph extension even though they both store information locally. When combined with the Knowledge Graph, it can also help maintain a clear trail of technical decisions and their rationale. For example I got stuck on a code migration and asked Goose to remember where we stopped, what we\'ve tried so far, and what we want to do next for when I start a new session.\\n\\n\\n## VS Code Extension: Your Favorite Editor, Connected\\n\\nOne of the biggest points in conversations with people especially around vibe coding, is finding ways to track what changes are being made. While version control is always recommended, sometimes I want to be able to stop or change direction before going too far. The [VS Code Extension](/docs/tutorials/vscode-mcp) alongside other features, allows me to preview the diff of my code changes before I commit them. \\n\\nI can choose to accept or refuse these changes, or tell Goose to try something else before any actual changes are made.\\n\\n\\n## The Power of Integration\\n\\nAs mentioned at the beginning of this post, the best thing about these MCP servers is how they plug into my existing workflow. I am able to:\\n\\n- Start a new session on Goose which opens the current folder as a project in VS Code.\\n- Start work on any changes and get any context I need from either the Knowledge Graph or from the internet using the Fetch extension.\\n- Any attempts at making changes takes my preferences from the Memory extension into account.\\n- I can then review these changes right in VS Code and either accept or reject them.\\n- And complete the task by asking Goose to create a pull request for me. \\n\\nThis is a simplified example of how I use these extensions together - I may not use all of them in every session, but having them available sure makes my workflow much smoother.\\n\\nWhat are your favorite MCP servers? How do you use them together? Share your experiences with us on [Discord server](https://discord.gg/block-opensource)!\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Top 5 MCP Servers I Use as a Developer with Goose Extensions\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/04/01/top-5-mcp-servers\\" />\\n  <meta property=\\"og:description\\" content=\\"These 5 MCP servers help me automate my workflow and make me a better developer.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/mcp-servers-cover-6994acb4dec5a3b33d10ea61f7609e4b.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Top 5 MCP Servers I Use as a Developer with Goose Extensions\\" />\\n  <meta name=\\"twitter:description\\" content=\\"These 5 MCP servers help me automate my workflow and make me a better developer.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/mcp-servers-cover-6994acb4dec5a3b33d10ea61f7609e4b.png\\" />\\n</head>"},{"id":"/2025/04/01/mcp-nondevs","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/04/01/mcp-nondevs","source":"@site/blog/2025-04-01-mcp-nondevs/index.md","title":"MCP Explained for Non-Developers","description":"Learn what Model Context Protocol (MCP) is and how anyone can use it to save time on tasks.","date":"2025-04-01T00:00:00.000Z","tags":[],"readingTime":5.19,"hasTruncateMarker":true,"authors":[{"name":"Tania Chakraborty","title":"Senior Technical Community Manager","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/tania"},"socials":{"linkedin":"https://www.linkedin.com/in/taniachakraborty/","x":"https://x.com/taniashiba","github":"https://github.com/taniashiba"},"imageURL":"https://avatars.githubusercontent.com/u/126204004?v=4","key":"tania"}],"frontMatter":{"title":"MCP Explained for Non-Developers","description":"Learn what Model Context Protocol (MCP) is and how anyone can use it to save time on tasks.","authors":["tania"]},"unlisted":false,"prevItem":{"title":"Top 5 MCP Servers I Use as a Developer with Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/01/top-5-mcp-servers"},"nextItem":{"title":"Community-Inspired Benchmarking: The Goose Vibe Check","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/31/goose-benchmark"}},"content":"![blog cover](mcp_nondevs.png)\\n\\nMCP this, MCP that, what exactly is it, and can you use them if you\'re not a developer? \ud83e\udd14\\n\\n\x3c!--truncate--\x3e\\n\\n## What is MCP?\\n\\nMCP stands for [Model Context Protocol](https://modelcontextprotocol.io/introduction), an open standard created by Anthropic.\\n\\nLet\'s say you\'re looking for ways to use AI at work to become more efficient and save as much time as possible. So you go off and learn about large language models (LLMs) like OpenAI or Claude, and start chatting with one. It\'s amazing being able to chat with AI and have it instantly answer questions or have it tell you how to do something, but how about getting the AI to do stuff for you?\\n\\nNow there are AI agents, or AI assistants, that can take actions and make decisions for you. But in order to have your AI agent interact with your systems, like Google Drive, Asana, or Slack, there wasn\'t a standard way to do it. At least not without figuring it out from scratch each time you needed your AI agent to work with what you need it to work with. That\'s super tedious.\\n\\nThat\'s exactly where MCP comes in. Best part is, you don\'t need to be a developer to start using them! MCP essentially allows you to give AI agents access to your external systems without having to code. You can think of MCP as the connector for a system and your AI agent, or like the USB-C of AI integrations.\\n\\n## MCP Servers You Should Try Right Now\\nSo what can you connect your AI agent to? MCP Servers! MCP servers give your agent access to your tools. With [over 3000 MCP servers](https://glama.ai/mcp/servers) you can connect to, here is your top list of popular MCP servers you should try:\\n\\n- **[Google Drive](/docs/tutorials/google-drive-mcp)**: File access and search capabilities for Google Drive\\n- **[YouTube Transcript](/docs/tutorials/youtube-transcript)**: Grab and work with YouTube video transcripts\\n- **[Google Maps](/docs/tutorials/google-maps-mcp)**: Location services, directions, and place details\\n- **[Tavily Web Search](/docs/tutorials/tavily-mcp)**: Web and local search using Tavily\'s Search API\\n- **[Asana](/docs/tutorials/asana-mcp)**: View asana tasks, projects, workspaces, and/or comments\\n- **[Speech](/docs/tutorials/speech-mcp)**: Real-time voice interaction, audio/video transcription, text-to-speech conversion and more\\n- **[GitHub](/docs/tutorials/github-mcp)**: Tools to read, search, and manage Git repositories\\n- **[Fetch](/docs/tutorials/fetch-mcp)**: Web content fetching and conversion for efficient LLM usage\\n\\nThis quick list should give you an idea of all the ways you can now use AI agents with your workflow. You can also explore community favorites in [handy MCP directories](https://dev.to/techgirl1908/my-favorite-mcp-directories-573n), and learn [how to check MCP servers are safe](/blog/2025/03/26/mcp-security) before installing.\\n\\nYou can also check out these [Goose tutorials](/docs/category/tutorials), showing you exactly how you can use some of these popular MCP servers with Goose, or use [Goose\'s Tutorial extension](/docs/tutorials/tutorial-extension) to get extra help walking you through using or building extensions.\\n\\n## Example MCP Prompts\\nNow that you\'ve caught a glimpse of some of the MCP servers that out there, how do you make sure you\'re using MCPs with AI agents the best you can? This is where prompts come in.\\n\\nPrompts are ultimately the text you input when interacting with an AI assistant, and prompts can range from super simple questions to detailed instructions! Here are some example prompts you can ask an AI agent like Goose right now that use some of the MCP servers mentioned above:\\n\\n### Google Maps\\n```\\nGoogle Maps: Track the live GPS location of driver ID #{driver_id}. Query Google Maps for real-time traffic data and adjust the estimated delivery time if delays exceed 5 minutes. If ETA changes, update the customer\'s live tracker and send an SMS notification. If the delay is greater than 20 minutes, check if another driver within a 1-mile radius can take over the delivery.\\n```\\n### YouTube Transcript\\n```\\nYouTube Transcript: Get the transcript from this youtube video [link to video]. Then, summarize it into a blog post.\\n```\\n### Google Drive\\n```\\nI have an important marketing budget review meeting in 30 minutes and I need your help getting prepared. I have several documents in my Google Drive from our previous meetings and planning sessions. Could you help me by:\\n\\n1. Finding all relevant documents about our marketing budget and performance\\n2. Giving me a quick summary of our Q1 performance\\n3. Highlighting the key decisions we need to make about the marketing automation tool and video production\\n4. Identifying any outstanding action items from our last meeting\\n```\\n### Asana\\n```\\nAsana: Create a new task in my Asana workspace called \'Review Q4 metrics\' and set the due date to next Friday. Then, find all tasks assigned to me that are due this week and summarize them.\\n```\\n### GitHub\\n```\\nGitHub: Create a new branch called hello-world in my angiejones/goose-demo repository. Update the README.md file to say \\"this was written by goose\\" and commit it. Open a pull request with your changes.\\n```\\n\\nTo see more examples just like this, along with the results you can get, check out this [Prompt Library](https://block.github.io/goose/prompt-library)! This is your central directory for discovering and using effective prompts with Goose.\\n\\n## The Possibilities Are Endless\\nWhile some are developed by official providers, a vast majority of MCP servers you see are actually developed by community members! Plus, because MCP is an open standard, anyone can build an MCP server for any resource. You could even use Goose to help you build one!\\n\\nHopefully now, instead of spending hours manually gathering data and creating your next marketing report, or manually sorting through your todo-backlog on a Monday, you will use MCP with Goose and have it done for you in minutes.\\n\\n*To learn more about using MCP servers and Goose, check out the [Goose documentation](https://block.github.io/goose/docs/category/getting-started), or join the [Block Open Source Discord](https://discord.gg/block-opensource) to connect with other open source community members.*\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"MCP Explained for Non-Developers\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/04/01/mcp-nondevs\\" />\\n  <meta property=\\"og:description\\" content=\\"Learn what Model Context Protocol (MCP) is and how anyone can use it to save time on tasks.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/mcp_nondevs-5ce7f39de923cab01de6e14e5dc06744.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"MCP Explained for Non-Developers\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Learn what Model Context Protocol (MCP) is and how anyone can use it to save time on tasks.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/mcp_nondevs-5ce7f39de923cab01de6e14e5dc06744.png\\" />\\n</head>"},{"id":"/2025/03/31/goose-benchmark","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/31/goose-benchmark","source":"@site/blog/2025-03-31-goose-benchmark/index.md","title":"Community-Inspired Benchmarking: The Goose Vibe Check","description":"See how open source AI models measure up in our first Goose agent benchmark tests","date":"2025-03-31T00:00:00.000Z","tags":[],"readingTime":19.54,"hasTruncateMarker":true,"authors":[{"name":"Alice Hau","title":"Machine Learning Engineer","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/alice"},"socials":{"linkedin":"https://www.linkedin.com/in/alice-hau/","github":"https://github.com/alicehau"},"imageURL":"https://avatars.githubusercontent.com/u/110418948?v=4","key":"alice"}],"frontMatter":{"title":"Community-Inspired Benchmarking: The Goose Vibe Check","description":"See how open source AI models measure up in our first Goose agent benchmark tests","authors":["alice"]},"unlisted":false,"prevItem":{"title":"MCP Explained for Non-Developers","permalink":"/goose/pr-preview/pr-2176/blog/2025/04/01/mcp-nondevs"},"nextItem":{"title":"Securing the Model Context Protocol","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/31/securing-mcp"}},"content":"import ImageCarousel from \'@site/src/components/ImageCarousel\';\\n\\n\\n![blog cover](goose-benchmark.png)\\n\\nWe\'ve been measuring Goose\'s performance with various AI models, including a variety of popular open-source models that can run locally on consumer hardware (RTX 4080, Mac M-series). We understand that many in our community value a fully open-source, local experience without relying on cloud services.\\n\\nThis blog shares our findings comparing open-source models against their closed counterparts, highlighting both current performance gaps and paths for future improvement. Our benchmark is still in its early stages, but we wanted to release it as a starting point for distinguishing models that exhibit stronger agentic capabilities by their ability to pilot Goose (distinct from reasoning or other capabilities often captured in other popular benchmarks).\\n\\n\\n\x3c!--truncate--\x3e\\n\\nOur evaluations are inspired by grassroots efforts we\'ve seen in communities like [r/LocalLlama](https://www.reddit.com/r/LocalLLaMA/). If you\'ve spent time there, you\u2019ve probably seen enthusiasts crowdsource model performance on standard tasks like \\"build a flappy bird game\\" or [create a rotating hexagon with a bouncing ball](https://www.reddit.com/r/LocalLLaMA/comments/1j7r47l/i_just_made_an_animation_of_a_ball_bouncing/)\\" to quickly compare model performance. \\n\\nThese community evals aren\'t the rigorous, peer-reviewed benchmarks that research labs publish in academic papers. However, they help provide quick, intuitive assessments of capabilities across different models and versions. \\n\\nIn this spirit, we\'re introducing our **Goose Vibe Check** leaderboard. \\n\\nThank you to the Ollama team for their help and support in our experimentation contributing to this blog! We used Ollama\u2019s [structured outputs](https://ollama.com/blog/structured-outputs) feature to enable our [toolshim implementation](https://block.github.io/goose/docs/guides/experimental-features/#ollama-tool-shim) (more below) and their recently released [context length parameter override](https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-specify-the-context-window-size) to enable testing on longer contexts.\\n\\n## Leaderboard\\n\\n| Rank | Model | Average Eval Score | Inference Provider |\\n|------|-------|-------------------|-------------------|\\n| 1 | claude-3-5-sonnet-2 | 1.00 | databricks (bedrock) |\\n| 2 | claude-3-7-sonnet | 0.94 | databricks (bedrock) |\\n| 3 | claude-3-5-haiku | 0.91 | databricks (bedrock) |\\n| 4 | o1 | 0.81 | databricks (bedrock) |\\n| 4 | gpt-4o | 0.81 | databricks (bedrock) |\\n| 6 | qwen2.5-coder:32b | 0.8 | ollama |\\n| 7 | o3-mini | 0.79 | databricks (bedrock) |\\n| 8 | qwq | 0.77 | ollama |\\n| 9 | gpt-4o-mini | 0.74 | databricks (bedrock) |\\n| 10 | deepseek-chat-v3-0324 | 0.73 | openrouter |\\n| 11 | gpt-4-5-preview | 0.67 | databricks |\\n| 12 | qwen2.5:32b | 0.64 | ollama |\\n| 13 | qwen2.5:14b | 0.62 | ollama |\\n| 14 | qwen2.5-coder:14b | 0.51 | ollama |\\n| 15 | deepseek-r1-toolshim-mistral-nemo* | 0.48 | openrouter |\\n| 16 | llama3.3:70b-instruct-q4_K_M | 0.47 | ollama |\\n| 17 | phi4-toolshim-mistral-nemo* | 0.46 | ollama |\\n| 18 | phi4-mistral-nemo | 0.45 | ollama |\\n| 19 | gemma3:27b-toolshim-mistral-nemo* | 0.43 | ollama |\\n| 20 | deepseek-r1-toolshim-qwen2.5-coder7b* | 0.42 | openrouter |\\n| 21 | llama3.3:70b-instruct-q8_0 | 0.41 | ollama |\\n| 22 | deepseek-r1:14b-toolshim-mistral-nemo* | 0.37 | openrouter |\\n| 23 | deepseek-r1-distill-llama-70b-toolshim-mistral-nemo* | 0.36 | ollama |\\n| 24 | phi4-toolshim-qwen2.5-coder7b* | 0.3 | ollama |\\n| 25 | mistral-nemo | 0.27 | ollama |\\n| 26 | deepseek-r1-distill-llama-70b-toolshim-qwen2.5-coder7b* | 0.26 | openrouter |\\n| 27 | llama3.2 | 0.25 | ollama |\\n| 28 | gemma3:27b-toolshim-qwen2.5-coder7b* | 0.24 | ollama |\\n| 29 | deepseek-r1:14b-toolshim-qwen2.5-coder7b* | 0.22 | ollama |\\n| 29 | gemma3:12b-toolshim-qwen2.5-coder7b* | 0.22 | ollama |\\n| 31 | mistral | 0.17 | ollama |\\n| 32 | gemma3:12b-toolshim-mistral-nemo* | 0.15 | ollama |\\n\\n> _Models with \'toolshim\' in their name indicate a Goose configuration using both a primary model and a secondary local Ollama model to interpret the primary model\'s response into appropriate tools for Goose to invoke. Low performance may be indicative of the shim performance rather than the base model itself. We use toolshims for select models because all evaluations in this experiment require tool use capabilities, but not all models in our experiment natively support tool calling._\\n\\n## Open Source Model Details\\n\\n| Rank | Model                                    | Model Params                      | Quantization |\\n|-----|-------------------------------------------|-----------------------------------|-------------|\\n| 1   | qwen2.5-coder:32b                          | 32B                               | Q4_K_M      |\\n| 2   | qwq                                        | 32B                               | Q4_K_M      |\\n| 3   | deepseek-chat-v3-0324                       | 671B total, 37B active             | -           |\\n| 4   | qwen2.5:32b                                | 32B                               | Q4_K_M      |\\n| 5   | qwen2.5:14b                                | 14B                               | Q4_K_M      |\\n| 6   | qwen2.5-coder:14b                           | 14B                               | Q4_K_M      |\\n| 7   | deepseek-r1-toolshim-mistral-nemo            | 671B total, 37B active             | fp8         |\\n| 8   | llama3.3:70b-instruct-q4_K_M                 | 70B                               | Q4_K_M      |\\n| 9   | phi4-toolshim-mistral-nemo                   | 14B                               | Q4_K_M      |\\n| 10  | phi4-mistral-nemo                           | 14B                               | Q4_K_M      |\\n| 11  | gemma3:27b-toolshim-mistral-nemo             | 27B                               | Q4_K_M      |\\n| 12  | deepseek-r1-toolshim-qwen2.5-coder7b         | 671B total, 37B active             | fp8         |\\n| 13  | llama3.3:70b-instruct-q8_0                   | 70B                               | Q8_0        |\\n| 14  | deepseek-r1:14b-toolshim-mistral-nemo         | 14B                               | Q4_K_M      |\\n| 15  | deepseek-r1-distill-llama-70b-toolshim-mistral-nemo | 70B                      | -           |\\n| 16  | phi4-toolshim-qwen2.5-coder7b                | 14B                               | Q4_K_M      |\\n| 17  | mistral-nemo                                | 12B                               | Q4_0        |\\n| 18  | deepseek-r1-distill-llama-70b-toolshim-qwen2.5-coder7b | 70B                 | -           |\\n| 19  | llama3.2                                    | 3B                                | Q4_K_M      |\\n| 20  | gemma3:27b-toolshim-qwen2.5-coder7b          | 27B                               | Q4_K_M      |\\n| 21  | deepseek-r1:14b-toolshim-qwen2.5-coder7b     | 14B                               | Q4_K_M      |\\n| 21  | gemma3:12b-toolshim-qwen2.5-coder7b          | 12B                               | Q4_K_M      |\\n| 23  | mistral                                     | 7B                                | Q8_0        |\\n| 24  | gemma3:12b-toolshim-mistral-nemo             | 12B                               | Q4_K_M      |\\n\\n\\n\\n![Open Model Performance by Parameter Size](model_sizes_vs_score.png)\\n\\n   > _This chart presents a view of open model performance across different parameter sizes. In the 15-32B category, we see particularly impressive results from models like qwen2.5-coder:32b (0.80) and qwq (0.77). The chart also highlights the performance gap between models with native tool calling capabilities versus those requiring toolshim implementation (shown with dotted lines), a gap which appears consistent across all size categories. This suggests that native tool calling capabilities significantly impact performance on agentic tasks. With targeted improvements in tool calling capabilities, larger open models could potentially close the performance gap with closed-source alternatives in agentic settings._\\n\\n\\n\\n![Token Usage vs Scores](tokens_vs_score.png)\\n\\n   > _This scatterplot shows Claude models achieving top scores (0.9+) regardless of token usage, while open source models like qwen2.5-coder:32b perform well with moderate token consumption. Toolshimmed models consistently score lower, suggesting the toolshims are not very effective at closing the gap in native tool support between models. Higher token consumption up to a point appears to generally improve performance._\\n\\n\\n\\n\\n![Tool Calls vs Scores](tool_calls_vs_score.png)\\n\\n> _Models with either too few or excessive tool calls score lower, indicating effective tool utilization - not just frequency - correlates with improved performance. Toolshimmed models for the most part invoke fewer tool calls, suggesting that the toolshims are not sufficient in their current implementation to make models effective at correctly calling the right tools._\\n\\n\\n\\n\\n## Key Results\\n\\n1. **Closed models currently lead**: Closed source models like Claude and GPT models still generally lead open source alternatives in agentic tasks.\\n\\n2. **Promising open challengers**: Models like the Qwen series and DeepSeek-v3 show significant promise among open source alternatives, but they have not yet reached the consistency and reliability of closed models across all tasks.\\n\\n3. **Token efficiency matters**: Some open models can achieve good performance while using fewer tokens, which can translate to faster task completion times and potentially lower cost. Claude-3-7-sonnet exhibits strong performance alongside claude-3-5-sonnet-2, but at much greater token usage.\\n\\n4. **Tool calling is crucial but not as reliable in open source models today**: Effective tool calling remains a significant differentiator in agentic model performance. Open source models still struggle with generating structured tool calls reliably, limiting their effectiveness on complex tasks.\\n\\n5. **More comprehensive and complex eval tasks are needed to further stratify the top performers:** Our current evaluation suite, consisting of only eight tasks (ran 3x), may be too limited to effectively differentiate top-performing models. Several models clustered around similar scores in the .77-.81 range, likely due to the simplicity of the tasks, which require minimal complex reasoning. Expanding the evaluations to include more sophisticated tasks would provide further stratification and allow the models to better showcase their more or less advanced capabilities.\\n\\n\\n## Approach and Methodology\\n\\nWe developed a compact suite of well-scoped evaluations to establish current performance baselines. While the tasks are relatively simple, they already meaningfully stratify model performance. Unlike benchmarks that focus primarily on text generation (e.g., question answering, code generation), our evaluations emphasize **tool calling capabilities** \u2014 a core component of what makes Goose a powerful agent.\\n\\nTool calling enables models to interact with [MCP extensions](https://github.com/modelcontextprotocol/servers) and make API calls, expanding Goose\'s functionality beyond the base models. In many cases, tasks required multiple chained tool calls to reach completion. For instance, modifying a file involves finding it in your filesystem, viewing its contents, and then updating it. Each step must be executed correctly to complete the task effectively.\\n\\n### Evaluation Suites\\n\\nOur evaluations are defined in the [Goose repository](https://github.com/block/goose/tree/main/crates/goose-bench/src/eval_suites) (PRs welcome to add additional evals!) and are grouped into two categories:\\n\\n#### Core Suite\\nThese evals focus on certain tasks fundamental to developer workflows:\\n- **Create a file**: Generate and save a new file\\n- **List files**: Access and display directory contents\\n- **Developer Search/Replace**: Search through a large file and make several replacements\\n\\n#### Vibes Suite \\nDesigned as a \\"vibe check\\", these tasks quickly reveal how well models perform with Goose on a broad variety of tasks. Some, like the Flappy Bird and Goose Wiki tasks are straightforwardly visually inspectable, making it easy to eyeball outputs across models:\\n\\n- **Blog summary**: Fetch a blog post and summarize key points\\n- **Flappy Bird**: Implement the game in Python 2D\\n- **Goose Wiki**: Create a Wikipedia-style webpage about Goose\\n- **Restaurant research**: Search for the best Sichuanese restaurants in NYC\'s East Village\\n- **Squirrel census**: Perform data analysis on a CSV file\\n\\nThis initial set of evaluations represents a carefully curated selection of manually designed tasks, chosen to highlight key strengths and weaknesses of models when integrated with Goose. However, this is just the beginning! Our goal is to continuously expand the Goosebench evaluation suite with high-quality, targeted tasks that provide deeper insights into model performance with Goose.\\n\\n### Evaluation Methodology\\n\\nEach model was tested on the above **8 tasks, with 3 runs per task**, (totaling **24 runs per model**):\\n\\n- Each evaluation consisted of a single turn prompt to Goose. While this benchmark focuses on single turn execution, future evaluations may assess multi-turn interactions and iterative improvement\\n- Goose was required to autonomously complete the task using tool execution loops without user intervention\\n- If Goose halted execution and asked the user for more guidance (e.g., \\"I am going to write the following contents to the file. Should I continue?\\"), this was considered the end of task completion. In such cases, Goose may not have successfully completed the task as measured by our evaluation framework, even if it was on the right track.\\n- To account for output variability, each evaluation was run three times per model, allowing multiple chances for success.\\n\\n### Scoring and Evaluation Criteria\\n\\nWe calculate each model\'s leaderboard score by averaging its performance across all evaluation tasks. For each task, we run the model three times and normalize each run\'s score to a 0-1 scale. The model\'s task score is the average of these three runs. The final leaderboard score is the average of all task scores for that model.\\n\\nEach evaluation is scored on a mix of criteria tailored to the specific task:\\n\\n1. **Tool Call Execution**: Did the model make the correct tool calls to complete the task?\\n\\n2. **LLM as a Judge** (where applicable): Some evaluations used GPT-4o to assess response quality on a 0-2 scale. In these cases, we generated 3 GPT-4o assessments, took the most common score among them, and ran a fourth assessment if needed to break a tie to get the final score.\\n   - 0 points: Incorrect or fundamentally flawed\\n   - 1 point: Partially correct, but with issues\\n   - 2 points: Fully correct and well executed\\n\\n3. **Task Specific Criteria**: Different tasks required different checks, such as:\\n   - Correct output formatting (e.g., markdown, output to file)\\n   - Expected answers (e.g., correct insights in data analysis)\\n   - Valid implementation (e.g., valid Python code)\\n\\nSome evaluations, like code execution or file creation, have clear pass/fail criteria, similar to unit tests. Others, such as blog summarization or restaurant research, require qualitative judgment rather than strict correctness. To assess both objective and open-ended tasks, we combine task-specific criteria, tool call verification, and (where applicable) LLM as a judge scoring.\\n\\nTo assess both objective and open-ended tasks, we combine task-specific criteria, tool call verification, and (where applicable) LLM-as-a-judge scoring. This approach maintains rigor where correctness is well-defined while allowing for nuanced evaluation of subjective outputs.\\n\\nOur goal is to provide a directional signal of model performance rather than absolute accuracy, balancing concrete and qualitative criteria.\\n\\n\\nAdditionally, we tracked:\\n\\n1. **Token Efficiency**: Measures total tokens used in successful runs, providing insight into model efficiency and inference speed.\\n\\n2. **Duration**: Time to execute the task. This is not reflected in the leaderboard as it is significantly affected by differences across model inference providers and hardware.\\n\\n### Manual Inspection and Observations of Results\\n\\nWe manually inspected a handful of results to assess quality. Given the scale (768 total runs across 32 models), full manual validation of every evaluation run was infeasible. Key takeaways from our inspections:\\n\\n- LLM-as-a-judge was reliable at identifying fully incorrect answers (0 points), but distinguishing between 1 and 2 points was more subjective.\\n\\n- Some tasks (e.g., blog summarization, restaurant searches) lacked automated factual verification. The evaluation framework could confirm whether a tool was called (e.g., web search executed) and the LLM judge could assess the instruction following to some degree, but our system overall had no way of verifying if the responses were factually correct.\\n\\n- Tool execution failures were a key source of performance variation, highlighting the importance of agentic capabilities in real-world AI tasks. A model might generate the correct output in chat, but if it fails to subsequently execute the right tools\u2014such as writing the output to the right file as instructed by the user\u2014the task is incomplete. This underscores the need for models to reliably perform multi-step actions autonomously, not just generate accurate responses.\\n\\n\\n## Technical Challenges with Open Models\\n\\n### Context Length Limitations\\n\\nA key limitation we encountered early on in our experimentation was the default context length in Ollama\'s OpenAI-compatible endpoint (2048 tokens), which proved insufficient for most interactive agent scenarios. \\n\\nOur system prompt alone consumes about 1,000 tokens, leaving limited space for user queries, context, and tool responses. This restriction hampers the model\'s ability to manage long-running or complex tasks without losing essential context. While quantization (e.g., many Ollama models default to 4-bit) can reduce memory usage, it can also degrade performance. \\n\\nHowever, we did not extensively explore the impact of different quantization levels. Fortunately, during our work, Ollama introduced an override that allowed us to increase the context length, helping to mitigate this limitation in our experiments.\\n\\n\\n### Tool Calling Inconsistencies Across Models\\n\\nDifferent models have varying expectations for tool calling formats. For instance, Ollama requires JSON, while others like Functionary use XML. This lack of standardization poses integration challenges for inference providers, who must adapt the tool calling mechanisms for each model. \\n\\nWe observed performance fluctuations based on the model host and input/output formatting, highlighting the need for standardized tool calling formats in model training.\\nFor models without native tool calling capabilities, we developed a \\"toolshim\\"\u2014an interpretive layer that translates a model\'s output into the appropriate tool calls.\\n\\nThis approach enables models like DeepSeek and Gemma to perform basic tool actions, though performance remains limited. None of the models configured with the toolshim greater than a 41% success rate in our experiments. Future improvements may focus on fine-tuning these shims for better handling of agentic tasks, helping to reduce inconsistencies across models in tool call generation.\\n\\n### \u201cToolshims\u201d to bridge the gap?\\n\\nWe developed a \\"toolshim\\" as an experimental feature to enable models lacking native tool calling support (e.g., DeepSeek, Gemma3, Phi4) to interact with external tools. The toolshim pairs these models with a smaller, local model (e.g., mistral-nemo, qwen2.5-coder 7b), which is tasked with translating the primary model\u2019s natural language responses into the appropriate tool calls for Goose to invoke. The local model is guided by Ollama\u2019s structured outputs feature to enforce proper formatting for tool call generations.\\n\\nHowever, this solution has limited performance due to:\\n\\n- **Instruction-following limitations:** The smaller models used typically have less robust instruction-following ability especially for longer inputs, making them prone to inaccuracies when parsing the primary model\'s output into the correct tool calls. We also found the shim models to be quite sensitive to prompting.\\n\\n- **Structured output interference:** Ollama\u2019s structured output feature influences the model\u2019s token sampling process, where the output is impacted by the model\u2019s fundamental ability to extract information and generate JSON appropriately.\\n\\nDespite these challenges, there could be potential in fine-tuning these toolshim models to specifically optimize them for tool call generation. \\nIf you\u2019d like to try out the toolshim, check out our [documentation](https://block.github.io/goose/docs/guides/experimental-features)\\n\\n## Practical Advice for Local Model Users\\n\\nFor those running a local, open-source AI experience with Goose, here are some key recommendations based on our testing:\\n\\n### Optimize Context Length\\n\\nEnsure your model has enough context length to avoid running out of space in the context window. For Ollama, you can adjust the context length via an environment variable:\\n\\n```bash\\nOLLAMA_CONTEXT_LENGTH=28672 ollama serve\\n```\\n\\nYou can also set the context length as a parameter in Ollama by updating the Modlfile with your desired context length and running `ollama create`.\\n\\n### Be Aware of Quantization Levels\\n\\nDifferent quantization levels (4-bit, 8-bit, and 16-bit) have distinct impacts on performance:\\n\\n- **4-bit:** Offers maximum compression with minimal memory requirements but may degrade quality.\\n- **8-bit:** A balanced option for most consumer hardware, providing good performance and reasonable quality.\\n- **16-bit:** Higher quality but requires significantly more memory, which may limit performance on lower-end hardware.\\n\\nOllama defaults to 4-bit quantization in most cases, but for tasks requiring more complex reasoning or tool usage, testing with higher quantization levels (e.g., 8-bit) may improve performance.\\n\\n\\n### Prompting Matters for Smaller Models\\n\\nSmaller models are more sensitive to prompt variations and often require more explicit instructions due to their limited capacity to infer. To achieve optimal performance, tasks may need to be broken down further, reducing ambiguity and limiting the range of possible responses. \\n\\n### Hardware Considerations\\n\\nWe ran these models with a variety of inference providers (local and hosted) and hardware configurations including Apple M1, NVIDIA RTX 4080, NVIDIA RTX 4090, and NVIDIA H100. Due to the mix of hardware, we did not include measurements of task duration in the benchmark given the expected variability in inference performance driven by the underlying hardware.\\n\\n#### GPU Backends\\n\\nDepending on your hardware, different GPU acceleration backends offer varying levels of performance:\\n\\n- **CUDA (NVIDIA GPUs)**: Currently offers the best performance and compatibility for running LLMs locally. Most open models and inference frameworks are optimized for CUDA first.\\n\\n- **Metal (Apple Silicon)**: Provides good acceleration on Mac devices with M-series chips. While not as fast as high-end NVIDIA GPUs, recent optimization work has made Metal increasingly viable for running 7B-13B models.\\n\\n- **ROCm (AMD GPUs)**: Support is improving but still lags behind CUDA. If you have a compatible AMD GPU, you may expect to see some performance limitations and compatibility issues with certain models and quantization methods.\\n\\n\\n\\n#### CPU/GPU Memory Management\\n\\nOllama helps distribute model layers across CPU and GPU memory, allowing you to run larger models than would fit entirely in your GPU VRAM. However, be aware of:\\n\\n- **Data movement overhead**: When a model doesn\'t fit entirely in GPU memory, constant data movement between CPU and GPU can significantly impact performance\\n- **GPU utilization**: Models that fit entirely in GPU memory will perform dramatically better than those that require CPU offloading\\n\\n\\n### Considering Cloud-Hosted Open Models?\\n\\nIf using a cloud service like OpenRouter to try larger open-weight models (e.g., LLaMA 3 70B or Qwen), be aware that performance may vary depending on which hosted inference provider you\'re using. \\n\\nDifferent providers might:\\n\\n- Quantize models on the backend without clear disclosure\\n- Implement different integration patterns that affect model performance, especially with tool calling\\n- Have different hardware configurations affecting speed and reliability\\n\\nWe recommend experimenting with different hosted inference providers to see which works best for your specific use cases. OpenRouter for example lets you [specify the provider](https://openrouter.ai/docs/features/provider-routing) you want to route your requests to.\\n\\n## Run Your Own Benchmarks\\n\\nWe encourage the community to conduct their own benchmarks with various hardware setups and configurations to help deepen our understanding of how Goose performs across different setups. We also welcome contributions of additional evals to GooseBench to broaden our coverage. \\n\\nWe are currently cleaning up our code and  working on some quality of life improvements to make the process of running evals and reproducing these results more streamlined, and will share those when ready (next few weeks)! \\n\\nSpecial thanks to our contributors, Zaki and Marcelle, for their work on GooseBench, which enabled this experimentation.\\n\\n\\n## Future Work\\n\\nAs AI capabilities continue to evolve, we aim to systematically expand our evaluation framework to capture a broader range of use cases. We hope to benchmark models on a wider swath of consumer-grade hardware to better understand system requirements, execution times, and the impact of different quantization levels on performance.\\n\\nWe also plan to introduce vision-oriented evaluations, particularly for multimodal models with Goose. These will assess image processing, multimodal reasoning, and visual tool interactions, helping us measure how well models integrate and perform across different modalities.\\n\\nIn addition, we seek to develop evaluations tailored to non-developer workflows and tasks. This will provide insights into how Goose and AI models can serve a wider range of users beyond technical audiences.\\n\\nFinally, we see value in testing long-context retention and multi-turn interactions to evaluate model performance in complex, sustained conversations.\\n\\n\\n## Result Eval Examples\\n\\n### Flappy Bird\\nFor runs that successfully created a working flappy bird game with pygame, here are the gifs of playing the games:\\n\\n\\n\\n<ImageCarousel id=\\"flappy\\" width=\\"40%\\" images={[\\n\\n  require(\'./flappy_bird_carousel/claude-3-5-haiku.gif\').default,\\n  require(\'./flappy_bird_carousel/claude-3-5-sonnet-2.gif\').default,\\n  require(\'./flappy_bird_carousel/claude-3-7-sonnet.gif\').default,\\n  require(\'./flappy_bird_carousel/deepseek-chat-v3-0324.gif\').default,\\n  require(\'./flappy_bird_carousel/deepseek-r1-toolshim-mistral-nemo.gif\').default,\\n  require(\'./flappy_bird_carousel/gpt-4-5-preview.gif\').default,\\n  require(\'./flappy_bird_carousel/gpt-4o-mini.gif\').default,\\n  require(\'./flappy_bird_carousel/gpt-4o.gif\').default,\\n  require(\'./flappy_bird_carousel/o1.gif\').default,\\n  require(\'./flappy_bird_carousel/o3-mini.gif\').default,\\n  require(\'./flappy_bird_carousel/qwen2.5-coder-32b.gif\').default,\\n  require(\'./flappy_bird_carousel/qwq.gif\').default,\\n ]}\\n names={[\\n    \\"claude-3-5-haiku\\",\\n    \\"claude-3-5-sonnet-2\\",\\n    \\"claude-3-7-sonnet\\",\\n    \\"deepseek-chat-v3-0324\\",\\n    \\"deepseek-r1-toolshim-mistral-nemo\\",\\n    \\"gpt-4-5-preview\\",\\n    \\"gpt-4o-mini\\",\\n    \\"gpt-4o\\",\\n    \\"o1\\",\\n    \\"o3-mini\\",\\n    \\"qwen2.5-coder-32b\\",\\n    \\"qwq\\"\\n  ]} />\\n\\n\\n### Wiki Pages\\n\\nFor runs that successfully created an index.html for the Wiki page task, here\u2019s what the rendered outputs look like: Wiki pages Missing results are for models that did not successfully write to an index.html file. For example, they may have outputted the code to write in chat and asked the user to implement that code in an index.html file rather than written to the file themselves.\\n\\n\\n\\n\\n<ImageCarousel id=\\"wiki\\" width=\\"80%\\" images={[\\n\\n   require(\'./wiki_pages_carousel/gemma3.27b-toolshim-mistral-nemo.png\').default,\\n   require(\'./wiki_pages_carousel/claude-3.5-haiku.png\').default,\\n   require(\'./wiki_pages_carousel/claude-3.5-sonnet-2.png\').default,\\n   require(\'./wiki_pages_carousel/claude-3.7-sonnet.png\').default,\\n   require(\'./wiki_pages_carousel/deepseek-chat-v3-0324.png\').default,\\n   require(\'./wiki_pages_carousel/deepseek-r1-distill-llama-70b-toolshim-mistral-nemo.png\').default,\\n   require(\'./wiki_pages_carousel/gpt-4.5-preview.png\').default,\\n   require(\'./wiki_pages_carousel/gpt-4o-mini.png\').default,\\n   require(\'./wiki_pages_carousel/gpt-4o.png\').default,\\n   require(\'./wiki_pages_carousel/llama3.3.70b-instruct-q4_K_M.png\').default,\\n   require(\'./wiki_pages_carousel/llama3.3.70b-instruct-q8_0.png\').default,\\n   require(\'./wiki_pages_carousel/mistral-nemo_index.png\').default,\\n   require(\'./wiki_pages_carousel/o1.png\').default,\\n   require(\'./wiki_pages_carousel/o3-mini.png\').default,\\n   require(\'./wiki_pages_carousel/phi4-toolshim-mistral-nemo.png\').default,\\n   require(\'./wiki_pages_carousel/phi4-toolshim-qwen2.5-coder7b.png\').default,\\n   require(\'./wiki_pages_carousel/qwen2.5-coder.14b.png\').default,\\n   require(\'./wiki_pages_carousel/qwen2.5-coder.32b.png\').default,\\n   require(\'./wiki_pages_carousel/qwen2.5.14b.png\').default,\\n   require(\'./wiki_pages_carousel/qwen2.5.32b.png\').default,\\n   require(\'./wiki_pages_carousel/qwq.png\').default\\n   ]} \\n   \\n   names={[\\n   \\"gemma3.27b-toolshim-mistral-nemo\\",\\n   \\"claude-3.5-haiku\\",\\n   \\"claude-3.5-sonnet-2\\",\\n   \\"claude-3.7-sonnet\\",\\n   \\"deepseek-chat-v3-0324\\",\\n   \\"deepseek-r1-distill-llama-70b-toolshim-mistral-nemo\\",\\n   \\"gpt-4.5-preview\\",\\n   \\"gpt-4o-mini\\",\\n   \\"gpt-4o\\",\\n   \\"llama3.3.70b-instruct-q4_K_M\\",\\n   \\"llama3.3.70b-instruct-q8_0\\",\\n   \\"mistral-nemo\\",\\n   \\"o1\\",\\n   \\"o3-mini\\",\\n   \\"phi4-toolshim-mistral-nemo\\",\\n   \\"phi4-toolshim-qwen2.5-coder7b\\",\\n   \\"qwen2.5-coder.14b\\",\\n   \\"qwen2.5-coder.32b\\",\\n   \\"qwen2.5.14b\\",\\n   \\"qwen2.5.32b\\",\\n   \\"qwq\\"\\n   ]}/>\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Community-Inspired Benchmarking: The Goose Vibe Check\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/31/goose-benchmark\\" />\\n  <meta property=\\"og:description\\" content=\\"See how open source AI models measure up in our first Goose agent benchmark tests\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/goose-benchmark-d9726c203290ef892fe3fe3adc7d898f.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Community-Inspired Benchmarking: The Goose Vibe Check\\" />\\n  <meta name=\\"twitter:description\\" content=\\"See how open source AI models measure up in our first Goose agent benchmark tests\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/goose-benchmark-d9726c203290ef892fe3fe3adc7d898f.png\\" />\\n</head>"},{"id":"/2025/03/31/securing-mcp","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/31/securing-mcp","source":"@site/blog/2025-03-31-securing-mcp/index.md","title":"Securing the Model Context Protocol","description":"Building secure and capable AI integrations with Model Context Protocol (MCP) at Block.","date":"2025-03-31T00:00:00.000Z","tags":[],"readingTime":9.875,"hasTruncateMarker":true,"authors":[{"name":"Alex Rosenzweig","title":"Staff Security Engineer","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/alex"},"socials":{"linkedin":"https://www.linkedin.com/in/alex-rosenzweig/","github":"https://github.com/shellz-n-stuff"},"imageURL":"https://media.licdn.com/dms/image/v2/C5103AQGrOYDDHn8z6g/profile-displayphoto-shrink_400_400/profile-displayphoto-shrink_400_400/0/1520985744775?e=1749081600&v=beta&t=krNAyF8rFRnInDfEZcO2TlLOHXWKMzt-fJdkgHVfFNs","key":"alex"}],"frontMatter":{"title":"Securing the Model Context Protocol","description":"Building secure and capable AI integrations with Model Context Protocol (MCP) at Block.","authors":["alex"]},"unlisted":false,"prevItem":{"title":"Community-Inspired Benchmarking: The Goose Vibe Check","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/31/goose-benchmark"},"nextItem":{"title":"Vibe Coding with Goose and the Speech MCP","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/28/vibe-coding-with-goose"}},"content":"![blog cover](securing-mcp.png)\\n\\n> _**Authors:** Alex Rosenzweig, Arihant Virulkar, Andrea Leoszko, Wes Ring, Mike Shema, F G, Alex Klyubin, Michael Rand, Zhen Lian, Angie Jones, Douwe Osinga, Mic Neale, Bradley Axen, Gelareh Taban_\\n\\n\\nAt Block, we\u2019ve been working hard to augment the capabilities of AI tooling by building \\"MCP Servers\\" which are designed to help make our Artificial Intelligence (AI) Agent codename goose more capable of interacting with the systems and tools we care about.\\n\\nBlock\u2019s Information Security (InfoSec) team has been heavily involved in this work and we wanted to capture our learnings in the space to help others. We expect there to be growing adoption and use cases for this including applying the technology in the security domain.\\n\\n\\n\x3c!--truncate--\x3e\\n\\n## What is the Model Context Protocol (MCP)\\n\\nModel Context Protocol (MCP) is a protocol [developed by Anthropic](https://docs.anthropic.com/en/docs/agents-and-tools/mcp), with input from Block engineers, that makes it easier to build integrations for agents to connect and use other tooling. Put simply, if you want AI to connect to SaaS solutions (e.g. Github, Jira),  CLI tools (e.g. AWS CLI) or your own custom applications you can write an MCP server and \\"teach\\" it how to correctly interact.\\n\\nThis has huge advantages as we can create deterministic, well defined interfaces that reduce the amount of \\"experimentation/brute force\\" required for agents to perform helpful tasks. \\n\\nA use case like \\"read this ticket from Jira and then clone the relevant github repo and implement the feature\\" is more likely to succeed if the agent doesn\u2019t have to work out how to interact with Jira, Github and the Git CLI.\\n\\nThis helps agents to spend time solving novel problems rather than burning tokens understanding well defined API specifications.\\n\\nThe following is example code from an MCP tool that integrates with an Snowflake API.\\n\\n```python\\n@mcp.tool()\\nasync def submit_feedback(\\n    feedback: str\\n) -> Dict[str, Union[str, int, List]]:\\n    \\"\\"\\"Submit feedback to the Snowflake team.\\n\\n    Args:\\n        feedback: Feedback message\\n\\n    Returns:\\n        Dictionary containing feedback status\\n    \\"\\"\\"\\n    return snowflake_client.submit_feedback(\\n        feedback_text=feedback\\n    )\\n```\\n\\n## MCP Misconceptions\\n\\nThere are some minor misconceptions around MCP, which is understandably exacerbated by some of the verbiage not accurately aligning with more analogous technologies. The biggest point of confusion is the terminology of \\"MCP Servers\\".\\n\\nUpon initially reviewing MCP, I noticed multiple references to \\"MCP Servers,\\" which led me to believe that integrating with them would require modifications to the application backend.\\n\\nHowever, these \\"servers\\" act as a client layer (either locally or remotely) to help the agent proxy function calls to an existing service, tool, API or RPC in a deterministic manner.\\n\\nWhen securing an MCP integration we need to think about two sets of communications:\\n\\n- How does the agent talk to the MCP Server?\\n- How does the MCP Server act as a client for the system it connects to?\\n\\nWe can model this by:\\n\\n- Treating the Agent as a non-deterministic client that can arbitrarily call tools provided by the MCP server. This is due to the fact that we don\u2019t know what prompts it will be provided.\\n- Treating the MCP Server as a Client Library for the utility/utilities it integrates into. The client type can vary (gRPC, REST, SOAP, CLI, etc.) but in practice, MCPs simply provide a codified way to execute an action.\\n\\nFor the former, we can lean into existing practices, understand the scope of access and what risks they present if used inappropriately. \\n\\nFor the latter, we can directly model it as a client for an external provider. This is a well understood pattern as client library generation is in no way new.\\n\\n![MCP Workflow](mcp-workflow.png)\\n\\n## How do we approach making it secure?\\n\\nUsing this mental model we can break MCP security into a few components:\\n\\n- Secure the Agents communication to the MCP\\n- Secure the MCPs connectivity to the tool/server\\n- Secure the identity of the user and the agent when talking to servers\\n- Secure the underlying host and supply chain\\n\\n### Securing Agentic Communications to MCP Servers\\n\\nIn the current operating model both the Agent and the MCP Server run on the \\"client side\\". \\n\\nHowever, the majority of agentic tools are integrated with LLMs provided by third parties. This has implications for data privacy and security. \\n\\nFor example if you expose an MCP interface that returns confidential data like Social Security Numbers ([what we at Block call DSL4 data](https://code.cash.app/dsl-framework)) then you run the risk of that data being exposed to the underlying LLM provider.\\n\\nA mitigation here is allowing MCP implementation to specify an allowlist of LLM providers that it can be integrated with as a configuration option. Having utilities to \\"tell\\" agents that can integrate with multiple models which models are allowed to invoke a given tool is a powerful primitive.\\n\\nBack to our SSN example, if we could specify that this tool can only be invoked by local LLM models and trust the Agent Client to enforce this we could prevent sensitive data from being transmitted to third party LLMs. As a further enhancement, being able to instruct agents not to share tool output with other MCPs would provide further control of dataflow.\\n\\n\\n### Securing MCP Communications to Tooling/Servers\\n\\nThis paradigm actually isn\u2019t new and we can lean into existing best practices for externally facing APIs. \\n\\nSpecifically, if we build our server side APIs with secure-by-design patterns already available through vetted frameworks already in-mind then we are already in a strong position as the MCP Server only acts as a client for these externally facing APIs and utilities.\\n\\nThe reason this paradigm isn\u2019t new is due to the fact that anyone can already interact with external APIs and tooling and likely will call the endpoints in unexpected ways. \\n\\nThis comes from the fact that LLMs interpret information in a manner that is different to human users, the protocol isn\u2019t implicitly allowing for agents to perform actions that users couldn\u2019t but LLMs may decide to perform actions that users wouldn\u2019t choose.\\n\\nWhere this **paradigm does shift** is when integrating with tooling not previously designed to be communicated with by all manner of clients. For example if an API was previously designed to only be communicated with by a specific client or implementation (such as a mobile APIs or internal tooling) then adopting MCP may lead to unexpected failure modes or security concerns.\\n\\nThis area is likely where Security Practitioners will need to concentrate further time and effort to limit integration scope to avoid damages in the event of a security attack against the underlying LLM or planning logic.\\n\\n\\n### Agent, Human and Device Identity\\n\\nIn our traditional model of Authentication (AuthN) and Authorization (AuthZ) it\u2019s common to tie an identity to a single point of abstraction such as a person or a business.\\n\\nThis field has organically been evolving towards pairing a services identity user identity abstraction with identification of client devices such as browsers and mobile phones. This is done to help reduce the prevalence of attacks caused by automation and inauthentic traffic such as account takeover attacks (ATO).\\n\\nWith the evolution of Agents performing actions on behalf of users we will need to evolve to be able to determine the combination of:\\n\\n1. The primary identity abstraction\\n2. The agent\u2019s identity\\n3. The device/location the agent is running from \\n\\nHaving consistent mechanisms for identifying usage in this manner allows companies to protect users from integrations with malicious agents and protect their platforms from attacks by unwanted agentic tooling.\\n\\nThe model context protocol itself has a [specification for OAuth](https://spec.modelcontextprotocol.io/specification/2025-03-26/basic/authorization/) that at the time of writing was a draft, but has since been released here. \\n\\nThis flow considers the following steps:\\n\\n1. Client/Agent initiates standard OAuth flow with MCP server\\n2. MCP server redirects user to third-party authorization server\\n3. User authorizes with third-party server\\n4. Third-party server redirects back to MCP server with authorization code\\n5. MCP server exchanges code for third-party access token\\n6. MCP server generates its own access token bound to the third-party session\\n7. MCP server completes original OAuth flow with Client/Agent\\n\\nThis is aligned with existing best practices but requires the MCPs themselves to have browser integrations/orchestration for OAuth to ensure they are able to redirect users effectively. \\n\\nA future enhancement we\u2019d love to see is requiring the agents to implement browser orchestration to provide an OAuth interface that MCPs themselves can integrate against and leverage. We believe this change would likely help standardise implementations and allow for protocol expansion to identify the agents and client alongside the user. \\n\\nHaving individual MCP implementations implement OAuth themselves is likely to lead to long term security and maintenance issues due to misimplementation or delays adopting future protocol enhancements.\\n\\n### Human in the loop for operational safety\\n\\nAt a certain point we may build enough trust in our agents to allow them to perform more dangerous operations. For these kinds of use cases we can likely lean on known good practices for change management.\\n\\nSpecifically, building server side solutions to alert the user to the expected changes and the agent performing them and seeking consent will likely be a critical primitive for APIs of the future. The goal of this would be to ultimately keep irreversible or hard to reverse actions gated behind human interaction or approval. \\n\\nFor example, for an agent tasked with writing IaC, this could be as simple as requesting a human approver before applying/deploying the IaC. \\n\\nIn client side agents this would improve data integrity in the event the underlying LLM hallucinated or was tampered with externally through malicious MCP or data sources. \\n\\nIn the latest release of the protocol, an enhancement we love is being able to [annotate a tool](https://github.com/modelcontextprotocol/specification/blob/9236eb1cbfa02c17ab45c83a7bdbe55c450070be/schema/2025-03-26/schema.ts#L730) to indicate to clients that tool actions are \\"readOnly\\" or \\"destructive\\". Using this to decide when to require a secondary approval from the user before performing a given action provides significantly better protections for users. \\n\\nWhile we encourage an LLM based processing step to check for potentially malicious commands, **having a deterministic aspect to higher risk commands in tandem ensures good access control is a more accurate way to provide protections**.\\n\\n### Securing the MCP Supply Chain\\n\\nAt this stage the majority of MCPs are being installed and run client side via commands like docker, uvx, pipx and npx. In practice this means when users install MCP based extensions they are providing arbitrary code execution privileges to the MCP Server.\\n\\nIn practice this presents a well documented and understood supply chain problem. How can we reduce risk associated with using third party code. The good news is that the same techniques still work including:\\n\\n1. Only install MCPs from trusted sources and are well maintained\\n2. Implement integrity checks and/or signing of artifacts where possible to ensure you\u2019re executing the expected code\\n3. Implement allow lists on enterprise agents to ensure users only use pre-validated MCPs\\n\\n## Conclusion\\n\\nMuch like agents are paving the way to allow LLMs to have more real-world utility MCP and similar protocols will continue to grow in adoption. \\n\\nWe believe that by contributing to open source projects early, sharing our learnings publicly, and building our own solutions that leverage MCP, Block can maintain security best practices from the deterministic world while continuing to evolve them with newer technologies.\\n\\nWe\u2019re excited to work on making this protocol more secure for users and developers alike and are looking forward to sharing how we\u2019ve used MCP for our own Security use-cases in the future.\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Securing the Model Context Protocol\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/31/securing-mcp\\" />\\n  <meta property=\\"og:description\\" content=\\"Building secure and capable AI integrations with Model Context Protocol (MCP) at Block.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/securing-mcp-5e475e91c0e621afa33e30b3d89ef065.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Securing the Model Context Protocol\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Building secure and capable AI integrations with Model Context Protocol (MCP) at Block.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/securing-mcp-5e475e91c0e621afa33e30b3d89ef065.png\\" />\\n</head>"},{"id":"/2025/03/28/vibe-coding-with-goose","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/28/vibe-coding-with-goose","source":"@site/blog/2025-03-28-vibe-coding-with-goose/index.md","title":"Vibe Coding with Goose and the Speech MCP","description":"Explore the new Speech MCP server that enables voice-controlled coding and natural conversation with your AI agent","date":"2025-03-28T00:00:00.000Z","tags":[],"readingTime":2.985,"hasTruncateMarker":true,"authors":[{"name":"Adewale Abati","title":"Staff Developer Advocate","url":"https://adewaleabati.com","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/adewale"},"socials":{"x":"https://x.com/ace_kyd","github":"https://github.com/acekyd"},"imageURL":"https://avatars.githubusercontent.com/u/4003538?v=4","key":"adewale"}],"frontMatter":{"title":"Vibe Coding with Goose and the Speech MCP","description":"Explore the new Speech MCP server that enables voice-controlled coding and natural conversation with your AI agent","authors":["adewale"]},"unlisted":false,"prevItem":{"title":"Securing the Model Context Protocol","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/31/securing-mcp"},"nextItem":{"title":"How to Determine If An MCP Server Is Safe","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/26/mcp-security"}},"content":"![blog cover](vibe-coding.png)\\n\\nImagine creating an app just by describing what you want out loud, like you\u2019re talking to a friend. That\u2019s the magic of vibe coding: turning natural language into working code with the help of an AI agent. And while typing a prompt gets the job done, saying it out loud hits different \ud83d\udd25 The new [Speech MCP server](https://block.github.io/goose/docs/tutorials/speech-mcp) has quite literally entered the chat.\\n\\n\x3c!--truncate--\x3e\\n\\nIn a recent [Wild Goose Case livestream](https://www.youtube.com/watch?v=Zey9GHyXlHY&ab_channel=BlockOpenSource), hosts [Ebony Louis](https://www.linkedin.com/in/ebonylouis/) and [Adewale Abati](https://www.linkedin.com/in/acekyd/) were joined by [Max Novich](https://www.linkedin.com/in/maksym-stepanenko-26404867) from Block\'s AI tools team, who demonstrated an exciting new extension - the [Speech MCP server](https://github.com/Kvadratni/speech-mcp). \\n\\nDuring the livestream, Max demonstrated this by creating an entire web application using only voice commands - no keyboard or mouse required. This resulted in a vibrant, animated webpage with 3D effects, synthwave aesthetics, and interactive elements, all created through natural conversation with Goose.\\n\\n<iframe class=\\"aspect-ratio\\" src=\\"https://www.youtube.com/embed/Zey9GHyXlHY?start=437&end=752\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\n\\n## The Speech MCP Server\\n\\n[Speech MCP](https://github.com/Kvadratni/speech-mcp) is an open source MCP server that enables voice interaction with AI agents like Goose. What makes it special is that it runs entirely locally on your machine, making it:\\n\\n- LLM agnostic\\n- Privacy-focused\\n- Cost-effective compared to cloud-based alternatives\\n- Accessible without internet connectivity\\n\\n### Key Features\\n\\n1. **Local Speech Processing**: Uses two main models:\\n   - Faster Whisper: An efficient method to convert speech to text\\n   - Coqui TTS: A Japanese-engineered text-to-speech model with 54 natural-sounding voices\\n\\n2. **Voice Selection**: Choose from 54 different voices with varying characteristics and personalities\\n\\n3. **Multi-Speaker Narration**: Generate and play conversations between multiple voices\\n\\n4. **Audio Transcription**: Convert audio/video content to text with timestamps and speaker detection\\n\\n## Live Demo Highlights\\n\\nDuring the demonstration, Max showcased several impressive capabilities:\\n\\n1. **Voice-Controlled Development**:\\n   - Created animated text effects\\n   - Implemented 3D transformations\\n   - Added synthwave aesthetics with gradients and grids\\n   - Integrated music controls\\n\\n2. **System Integration**:\\n   - Controlled applications like Discord using voice commands\\n   - Navigated file system and development environment\\n   - Generated and managed audio content\\n\\n3. **Natural Interaction**:\\n   - Fluid conversation with Goose\\n   - Real-time feedback and adjustments\\n   - Multi-voice narration for documentation\\n\\n## Getting Started\\n\\nTo try the Speech MCP server yourself:\\n\\n1. Install the required audio library (PortAudio):\\n   ```bash\\n   # For macOS\\n   brew install portaudio\\n   \\n   # For Linux\\n   apt-get install portaudio  # or dnf install portaudio\\n   ```\\n\\n2. Install the extension directly using the one-click [deep link install](goose://extension?cmd=uvx&&arg=-p&arg=3.10.14&arg=speech-mcp@latest&id=speech_mcp&name=Speech%20Interface&description=Voice%20interaction%20with%20audio%20visualization%20for%20Goose) in Goose\\n\\n\\n## Join the Development\\n\\nThe Speech MCP server is [open-source](https://github.com/Kvadratni/speech-mcp) and welcomes contributions. You can also connect with Max on [Discord](https://discord.gg/block-opensource) for questions and collaboration.\\n\\nVoice interactions with AI agents like Goose with the power and tools to act on instructions provides a different kind of vibe that makes the future feel closer than ever. Whether you\'re interested in vibe coding, accessibility improvements, or just want to feel a bit more like Tony Stark while getting Goose to pull a J.A.R.V.I.S, the Speech MCP server offers a glimpse into the future of human-AI collaboration - and it\'s available today.\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Vibe Coding with Goose and the Speech MCP\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/28/vibe-coding-with-goose\\" />\\n  <meta property=\\"og:description\\" content=\\"Explore the new Speech MCP server that enables voice-controlled coding and natural conversation with your AI agent.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/vibe-coding-b2efeed37ea43f4773da5f1ff96f4184.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Vibe Coding with Goose and the Speech MCP\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Explore the new Speech MCP server that enables voice-controlled coding and natural conversation with your AI agent.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/vibe-coding-b2efeed37ea43f4773da5f1ff96f4184.png\\" />\\n</head>"},{"id":"/2025/03/26/mcp-security","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/26/mcp-security","source":"@site/blog/2025-03-26-mcp-security/index.md","title":"How to Determine If An MCP Server Is Safe","description":"Before you plug your agent into just any MCP server, here\'s how to check if it\'s actually safe.","date":"2025-03-26T00:00:00.000Z","tags":[],"readingTime":2.715,"hasTruncateMarker":true,"authors":[{"name":"Ebony Louis","title":"Developer Advocate","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/ebony"},"socials":{"linkedin":"https://www.linkedin.com/in/ebonylouis/","x":"https://x.com/ebonyjlouis","github":"https://github.com/ebonylouis"},"imageURL":"https://avatars.githubusercontent.com/u/55366651?v=4","key":"ebony"}],"frontMatter":{"title":"How to Determine If An MCP Server Is Safe","description":"Before you plug your agent into just any MCP server, here\'s how to check if it\'s actually safe.","authors":["ebony"]},"unlisted":false,"prevItem":{"title":"Vibe Coding with Goose and the Speech MCP","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/28/vibe-coding-with-goose"},"nextItem":{"title":"Codename Goose Goes to Boston","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/21/goose-boston-meetup"}},"content":"![blog cover](mcpsafety.png)\\n\\n# How I Vet MCP Servers Before Plugging Them In\\n\\n[Model Context Protocol (MCP)](https://www.anthropic.com/news/model-context-protocol) servers are everywhere right now. Last time I checked there were **3,000 and counting**. Every day, a new one pops up, letting AI agents like Goose access files, query your Google Drive, search the web, and unlock all kinds of amazing integrations.\\n\\n\x3c!--truncate--\x3e\\n\\nAnd just when I thought things couldn\u2019t get any crazier, Zapier blessed us with an MCP server. That means your agent can now tap into over 8,000+ integrations.\\n\\nSo trust me, I know it\u2019s super tempting to want to plug your AI agent into everything and just _see_ what happens.\\n\\nBut hold on a minute, we can\u2019t afford to skip over security.\\n\\nWhen you connect to an MCP server, you\u2019re giving it access to your workflows, most times even your data. And a lot of these servers are community built, with little to no governance.\\n\\n## Here\u2019s What I Do Before I Trust an MCP Server\\n\\nAny time I\u2019m checking out a new MCP server to plug into Goose, I start with **[Glama.ai](https://glama.ai/mcp/servers)**.\\n\\nGlama is an all-in-one AI workspace, and it maintains one of the **most comprehensive and security-aware MCP server directories** that I\'ve seen. The servers listed are either community built or created by the actual companies behind the tools, like **Azure** or **JetBrains**.\\n\\nEach server gets a **report card**, so at a glance you can quickly assess whether it\u2019s solid or a little sketchy.\\n\\n## What Glama Scores\\n\\nHere\u2019s what Glama grades servers on:\\n\\n- \u2705 **Security** \u2013 Checks for known vulnerabilities in the server or its dependencies  \\n- \u2705 **License** \u2013 Confirms it\u2019s using a permissive open source license  \\n- \u2705 **Quality** \u2013 Indicates whether the server is running and functions as expected\\n\\nYou\u2019ll also see helpful context like how many tools the server exposes, whether it has a README file, when it was last updated, and whether it supports live previews through the MCP inspector tool.\\n\\nGlama doesn\'t just perform these checks once, they **revaluate servers regularly**, so if something breaks or a vulnerability gets introduced, the score updates automatically.\\n\\nHere\u2019s an example of a solid server: the **YouTube MCP server**, which lets Goose download and process videos to create summaries and transcripts.\\n\\n![YouTube MCP Score](youtubeMcp.png)\\n\\n>_All A\u2019s across the board\u2014**security, license, and quality**._  \\n\\nThat\u2019s exactly the kind of score I look for before I plug Goose into any server.\\n\\nSo please, **check before you connect**.\\n\\nA quick glance at an MCP directory like Glama can save you from crying on your office floor later. However, once you\u2019ve done your homework?\\n\\n**Have fun. Plug your agent in. Break things (safely). And vibe code with peace of mind.**\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"How to Determine If An MCP Server Is Safe\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/21/goose-vscode\\" />\\n  <meta property=\\"og:description\\" content=\\"Before you plug your AI agent into just any MCP server, here\'s how to check if it\'s actually safe.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/mcpsafety-87eb7ace7163a5edbe068ff75b79a199.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"How to Determine If An MCP Server Is Safe\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Before you plug your agent into just any MCP server, here\'s how to check if it\'s actually safe.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/mcpsafety-87eb7ace7163a5edbe068ff75b79a199.png\\" />\\n</head>"},{"id":"/2025/03/21/goose-boston-meetup","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/21/goose-boston-meetup","source":"@site/blog/2025-03-21-goose-boston-meetup/index.mdx","title":"Codename Goose Goes to Boston","description":"We hosted our first Goose Meetup in Boston, bringing together over 70 community members for lightning talks, hacking, and lively conversations about agentic systems and the future of MCPs.","date":"2025-03-21T00:00:00.000Z","tags":[],"readingTime":4.815,"hasTruncateMarker":true,"authors":[{"name":"Rizel Scarlett","title":"Staff Developer Advocate","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/rizel"},"socials":{"x":"https://x.com/blackgirlbytes","github":"https://github.com/blackgirlbytes"},"imageURL":"https://avatars.githubusercontent.com/u/22990146?v=4","key":"rizel"}],"frontMatter":{"title":"Codename Goose Goes to Boston","description":"We hosted our first Goose Meetup in Boston, bringing together over 70 community members for lightning talks, hacking, and lively conversations about agentic systems and the future of MCPs.","authors":["rizel"]},"unlisted":false,"prevItem":{"title":"How to Determine If An MCP Server Is Safe","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/26/mcp-security"},"nextItem":{"title":"Cracking the Code with VS Code MCP","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/21/goose-vscode"}},"content":"![blog cover](goose_goes_to_boston_banner.png)\\n\\n*Question: What happens when you bring 70+ AI enthusiasts, open source contributors, and curious learners together in one room?*\\n\\n**Answer: You get an electric night filled with great conversations, hands-on hacking, and mind-blowing insights into agentic systems.**\\n\\nThis week, we hosted our very first [Goose](https://block.github.io/goose) Meetup in Boston at the Cambridge Innovation Center. The turnout and energy exceeded all expectations! From first-time Goose users to seasoned AI engineers, attendees gathered to explore how Goose and the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) are shaping the future of AI automation.\\n\\n\x3c!--truncate--\x3e\\n\\n## Why We Hosted This Meetup\\n\\nAs our community continues to grow, we wanted to create a space where Goose enthusiasts could:\\n\\n\u2705 Network and meet like-minded technologists\\n\\n\u2705 Geek out over agentic systems and MCP\\n\\n\u2705 Learn through talks, demos, and hands-on hacking\\n\\nBoston has a thriving tech ecosystem, and it was incredible to see so many people come together to explore the future of AI agents.\\n\\n## In Case You Missed It\\n\\nAfter pizza and networking, we kicked off with two lightning talks that set the tone for the rest of the night:\\n\\n-  Ebony Louis, Developer Advocate at Block, Inc. delivered an engaging introduction to Goose, covering how to get started, its capabilities, and a hands-on demo that hooked the audience. Reflecting on the night, she shared:\\n\\n    > \\"There\u2019s no replacement for meeting in person and getting to talk to people face-to-face about where AI agents can take us. It was great to see the excitement of people who were using Goose for the first time, but it was even more productive to get questions and feedback about how we can work together to make the user experience better. AI is changing how we interact online, but this meetup was proof that it can bring us together in real life, too.\\"\\n\\n![Ebony takes the stage](ebony_preparing.jpg)\\n_Ebony preparing to take the stage_\\n\\n-  Alex Hancock, Senior Software Engineer at Block, Inc. and MCP Committee Member, followed up with a deep dive into Model Context Protocol architecture, breaking down how it powers agentic systems and what\u2019s next for the ecosystem. Alex was impressed by the enthusiasm in the room, sharing:\\n\\n    > \\"I was blown away by the attendance and the engagement. I had a lot of fun sticking around after the talks to answer questions and hear how people are thinking of using Goose and MCP.\\"\\n\\n![Alex speaking](alex_speaking.jpg)\\n_Alex sharing MCP insights with the audience_\\n \\nBoth talks were packed with insights, great humor, and interactive moments that had attendees excited for what was to come.\\n\\n## Moments We Loved \\n\\nSome of our favorite highlights from the night:\\n\\n\ud83d\udcde Goose made a live phone call to Hack.Diversity alum Eliana Lopez during the event. This was a fun moment that showcased Goose\'s real-world capabilities in automating everyday tasks.\\n\\n\\n{/* Video Player */}\\n<div style={{ width: \'100%\', maxWidth: \'800px\', margin: \'0 auto\' }}>\\n  <video \\n    controls \\n    width=\\"100%\\" \\n    height=\\"400px\\"\\n    playsInline\\n  >\\n    <source src={require(\'@site/static/videos/goose_makes_a_call.mp4\').default} type=\\"video/mp4\\" />\\n    Your browser does not support the video tag.\\n  </video>\\n</div>\\n\\n\ud83d\udcbb Attendees jumped into hands-on hacking, working on their own projects, experimenting with Goose, and sharing ideas in real time. To make sure everyone could participate, we provided OpenRouter credits, allowing attendees to run Goose without worrying about access barriers.\\n\\n\\n![Attendee hacking](attendee_hacking.jpg)\\n_Attendee hacking alongside Goose_\\n\\n\\n![Debugging with attendees](debugging_with_attendees.jpg)\\n_Rizel Scarlett debugging with meetup attendees_\\n\\n\\n\ud83d\udcac Riveting discussions sparked throughout the evening, including:\\n- Security for MCPs \u2014 How should we be thinking about security as agentic systems scale?\\n- The future of agentic systems \u2014 What are we now capable of, and what\u2019s next?\\n\\n![chatting with attendees](goose_boston_conversations.jpg)\\n_Alex chatting with attendees_\\n\\nFor Marcelle B., a Software Engineer at Block Inc., the meetup highlighted just how diverse the community was and reinforced the importance of making Goose accessible to everyone:\\n\\n    > \\"It was enlightening to see the variety of backgrounds of the attendees. For a few, Goose was an opportunity to bring the power of AI to bear on their project, and our meetup was a friendly place for them to dip their toes in. This gave me such empathy for the kind of product features and the level of rock-solid implementation we need in development, so that Goose can continue to be an empowering tool, rather than another AI product that you have to be some kind of \'insider\' to use.\\"\\n\\n## The Impact & What\u2019s Next\\n\\nThe excitement and engagement from this meetup proved just how much community-driven learning matters. Attendees loved the experience, with one person sharing:\\n\\n    > \\"Fantastic event\u2014well-paced, super friendly, learned a lot, and met great people. 10/10 would recommend!\\"\\n\\nThis event wouldn\u2019t have been the same without all of you. A huge shoutout to Hack.Diversity, Resilient Coders, Goose contributors, and Boston\u2019s tech scene for showing up, supporting, and making this meetup such a success!\\n\\n\\n![Goose Team in Boston](goose_team_in_boston.jpg)\\n_The Goose Team in Boston_\\n\\n\\n- If you\'re experiencing FOMO (the Fear of Missing Out) and want to join the next meetup, follow us on [social media](https://linktr.ee/blockopensource) to stay updated.\\n\\n- Bring a Goose meetup to your city! If you have a venue, reach out to us on [Discord](https://discord.gg/block-opensource)\u2014let\u2019s make it happen!\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Codename Goose Goes to Boston\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/21/goose-boston-meetup\\" />\\n  <meta property=\\"og:description\\" content=\\"We hosted our first Goose Meetup in Boston, bringing together over 70 community members for lightning talks, hacking, and lively conversations about agentic systems and the future of MCPs.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/goose_goes_to_boston_banner-3ef0eedeb9d3eac56907c0c5e615d919.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Codename Goose Goes to Boston\\" />\\n  <meta name=\\"twitter:description\\" content=\\"We hosted our first Goose Meetup in Boston, bringing together over 70 community members for lightning talks, hacking, and lively conversations about agentic systems and the future of MCPs.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/goose_goes_to_boston_banner-3ef0eedeb9d3eac56907c0c5e615d919.png\\" />\\n</head>"},{"id":"/2025/03/21/goose-vscode","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/21/goose-vscode","source":"@site/blog/2025-03-21-goose-vscode/index.md","title":"Cracking the Code with VS Code MCP","description":"Connect Goose directly to your code editor with this Visual Studio Code MCP.","date":"2025-03-21T00:00:00.000Z","tags":[],"readingTime":3.045,"hasTruncateMarker":true,"authors":[{"name":"Tania Chakraborty","title":"Senior Technical Community Manager","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/tania"},"socials":{"linkedin":"https://www.linkedin.com/in/taniachakraborty/","x":"https://x.com/taniashiba","github":"https://github.com/taniashiba"},"imageURL":"https://avatars.githubusercontent.com/u/126204004?v=4","key":"tania"}],"frontMatter":{"title":"Cracking the Code with VS Code MCP","description":"Connect Goose directly to your code editor with this Visual Studio Code MCP.","authors":["tania"]},"unlisted":false,"prevItem":{"title":"Codename Goose Goes to Boston","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/21/goose-boston-meetup"},"nextItem":{"title":"How I Use Goose to Plan My Week with Asana and Google Calendar MCPs","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/20/asana-calendar-mcp"}},"content":"![blog cover](vscodestream.png)\\n\\nWant to use Goose in VS Code? On the recent [Wild Goose Case livestream](https://www.youtube.com/watch?v=hG7AnTw-GLU&ab_channel=BlockOpenSource), hosts [Ebony Louis](https://www.linkedin.com/in/ebonylouis/) and [Adewale Abati](https://www.linkedin.com/in/acekyd/) were joined by [Andrew Gertig](https://www.linkedin.com/in/andrewgertig/), Engineering Lead at Cash App, as he demonstrated the new VSCode MCP and how it brings powerful Goose-assisted coding capabilities directly into VS Code.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is the VSCode MCP?\\nThe [VSCode MCP Server](https://github.com/block/vscode-mcp) and its companion [VSCode Extension](https://marketplace.visualstudio.com/items?itemName=block.vscode-mcp-extension) enable AI agents like Goose to interact with VS Code through the Model Context Protocol.\\n\\nAs Andrew explained during the stream, an MCP ([Model Context Protocol](https://modelcontextprotocol.io/introduction)) server acts as a proxy between a Large Language Model (LLM) and whatever applications or tools you want to access to, in this case, VS Code. Extensions are add-ons based on this protocol that provide a way to extend Goose\'s functionality for your workflow.\\n\\n```\\nvscode-mcp/\\n\u251c\u2500\u2500 server/    # MCP server implementation\\n\u2514\u2500\u2500 extension/ # VS Code extension\\n```\\n\\n## Key Features\\nVSCode MCP and VSCode Extension offer several powerful features for you to explore:\\n\\n**Intelligent Context Awareness**\\n\\nThe extension maintains synchronization between Goose and your VS Code environment to understand your project structure and make contextually relevant suggestions. During the live demo, this came in handy as Goose navigated complex codebases with precision.\\n\\n**Interactive Code Modifications**\\n\\nRather than making direct changes, the extension presents modifications through VS Code\'s diff tool. This ensures that no code changes happen without your explicit approval, allowing you to keep control over your codebase.\\n\\n**Progressive Complexity Handling**\\n\\nDuring the demo, the VSCode MCP seamlessly handled tasks ranging in complexity, from basic text modifications to implementing interactive features like animated emojis with mouse interactions.\\n\\n**Real-time Visual Feedback**\\n\\nDevelopers can see proposed changes in real-time with the diff view, making it easy to understand exactly what modifications Goose is suggesting before accepting them. This was demonstrated when an emoji\'s sizes visually while preserving existing functionality.\\n\\n## What\'s Next for VSCode MCP?\\nThe features don\'t end here. The team is actively exploring several exciting features to take VSCode MCP to the next level:\\n\\n- **Custom diff tool for granular control** - This means you will be able to be selective on specific parts of changes you want to accept or reject.\\n- **Smart navigation to specific code locations** - Imagine being able to ask Goose to take you directly to a function definition or a specific implementation.\\n- **Enhanced linting integration** - To help maintain code quality standards automatically, making it way easier to fix issues before production.\\n- **Terminal integration for command execution** - This would allow Goose to execute commands and display results right in your development environment.\\n- **Potential VS Code sidebar integration for Goose chat** - Andrew showed a quick preview of an early prototype showing Goose running directly inside VS Code.\\n\\n# Community and Contributing\\nThe project is open source, and welcomes contributions from the community. If you\'d like to support the project or directly contribute to it, you can check out [the VSCode MCP repo on GitHub](https://github.com/block/vscode-mcp), or [join the Block Open Source Discord](https://discord.gg/block-opensource) if you\'d like to ask the team any questions or start discussions.\\n\\nYou can also follow the [tutorial showing you how to integrate VS Code with Goose](https://block.github.io/goose/docs/tutorials/vscode-mcp).\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Cracking the Code in VS Code\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/21/goose-vscode\\" />\\n  <meta property=\\"og:description\\" content=\\"Connect Goose directly to your code editor with this Visual Studio Code MCP.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/vscodestream-74eafa34e7ae10cfb738feddecc98519.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Cracking the Code in VS Code\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Connect Goose directly to your code editor with this Visual Studio Code MCP.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/vscodestream-74eafa34e7ae10cfb738feddecc98519.png\\" />\\n</head>"},{"id":"/2025/03/20/asana-calendar-mcp","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/20/asana-calendar-mcp","source":"@site/blog/2025-03-20-asana-calendar-mcp/index.md","title":"How I Use Goose to Plan My Week with Asana and Google Calendar MCPs","description":"Use MCPs with Goose to automate task management and enhance productivity.","date":"2025-03-20T00:00:00.000Z","tags":[],"readingTime":2.15,"hasTruncateMarker":true,"authors":[{"name":"Angie Jones","title":"Head of Developer Relations","url":"https://angiejones.tech","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/angie"},"socials":{"linkedin":"https://www.linkedin.com/in/angiejones/","github":"https://github.com/angiejones","x":"https://x.com/techgirl1908"},"imageURL":"https://avatars.githubusercontent.com/u/15972783?v=4","key":"angie"}],"frontMatter":{"title":"How I Use Goose to Plan My Week with Asana and Google Calendar MCPs","description":"Use MCPs with Goose to automate task management and enhance productivity.","authors":["angie"]},"unlisted":false,"prevItem":{"title":"Cracking the Code with VS Code MCP","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/21/goose-vscode"},"nextItem":{"title":"AI Prompting 101: How to Get the Best Responses from Your AI Agent","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/19/better-ai-prompting"}},"content":"![blog cover](mcp-planner.png)\\n\\nMondays are overwhelming. A pile of unfinished tasks from last week, new priorities rolling in, and meetings scattered across the calendar. It\u2019s a lot \ud83d\ude29. Instead of manually sorting through my todos and figuring out where everything fits, I use a couple of handy MCP servers with Goose and let it figure out my week.\\n\\n\x3c!--truncate--\x3e\\n\\nThere\'s so many amazing MCP servers out there to make my work life better, including [Asana](https://github.com/roychri/mcp-server-asana) and [Google Calendar](https://www.pulsemcp.com/servers?q=google+calendar). I added these as Goose extensions, which means Goose can now can pull in my tasks, analyze them, and schedule them, all with one simple prompt:\\n\\n> _**Goose, pull all uncompleted tasks assigned to me in Asana. Group them by type of work to reduce context switching. Estimate how long each task will take. Then, schedule each task accordingly in my Google Calendar.  Make sure not to double book or overload any single day.**_\\n\\n\\n:::info\\nI used GPT-4o for this task\\n:::\\n\\nWith this prompt, Goose reviews my uncompleted tasks in Asana (note that I have my workspace, project, and user IDs stored in [memory](/docs/tutorials/memory-mcp)).\\n\\nRather than bouncing between different types of work, which is a productivity killer, Goose sorts my tasks into categories based on context. For example:\\n\\n* Writing-related tasks (blog posts, documentation, emails)\\n* Async collaboration (PR reviews, providing feedback)\\n* Technical work (coding, etc)\\n\\nBy grouping similar tasks, I can stay in the right headspace without constantly switching gears.\\n\\nGoose then estimates how long each task will take, the complexity of the task, and any deadlines. If I need to manually adjust something, I can, but it\u2019s usually pretty spot on.\\n\\nWith my tasks organized and estimated, Goose finds open time slots in my Google Calendar and automatically schedules them. It avoids my meetings and ensures I\u2019m not overloading any single day.\\n\\nWithin the first few minutes of the start of my week, my schedule is already mapped out, optimized for focus.\\n\\nThis has been so extremely helpful in increasing my productivity. Thanks, Goose! \ud83d\ude80\\n\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"MCP in Action: How I Use AI to Plan My Week with Goose, Asana, and Google Calendar\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/20/asana-calendar-mcp\\" />\\n  <meta property=\\"og:description\\" content=\\"Use MCPs with Goose to automate task management and enhance productivity.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/mcp-planner-761303c5ddcd5c79ed853536e3f87bcf.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"MCP in Action: How I Use AI to Plan My Week with Goose, Asana, and Google Calendar\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Use MCPs with Goose to automate task management and enhance productivity.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/mcp-planner-761303c5ddcd5c79ed853536e3f87bcf.png\\" />\\n</head>"},{"id":"/2025/03/19/better-ai-prompting","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/19/better-ai-prompting","source":"@site/blog/2025-03-19-better-ai-prompting/index.md","title":"AI Prompting 101: How to Get the Best Responses from Your AI Agent","description":"How to prompt your AI agent the right way.","date":"2025-03-19T00:00:00.000Z","tags":[],"readingTime":6.845,"hasTruncateMarker":true,"authors":[{"name":"Ebony Louis","title":"Developer Advocate","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/ebony"},"socials":{"linkedin":"https://www.linkedin.com/in/ebonylouis/","x":"https://x.com/ebonyjlouis","github":"https://github.com/ebonylouis"},"imageURL":"https://avatars.githubusercontent.com/u/55366651?v=4","key":"ebony"}],"frontMatter":{"title":"AI Prompting 101: How to Get the Best Responses from Your AI Agent","description":"How to prompt your AI agent the right way.","authors":["ebony"]},"unlisted":false,"prevItem":{"title":"How I Use Goose to Plan My Week with Asana and Google Calendar MCPs","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/20/asana-calendar-mcp"},"nextItem":{"title":"How Goose Catches AI Errors with Langfuse","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/18/goose-langfuse"}},"content":"![blog cover](prompt.png)\\n\\nRemember that saying, \\"it\u2019s not what you ask, but **how you ask**\\"?\\n\\nWhen I first started working with Goose as an AI agent, I was convinced there had to be one \u2018best\' prompting style. I spent so much time trying to figure out which one was superior, but the more I used Goose, the more I realized that couldn\'t be further from the truth. There isn\u2019t one _right_  way to prompt AI, but there are better approaches depending on what your end goal is.\\n\\nSo, let\u2019s go through **which prompt style works best for your specific needs**, and how you can use them to vibe code a little better with Goose.\\n\\n\x3c!--truncate--\x3e\\n\\n## Instruction-Based Prompting\\n\\nIf you\u2019re not a developer or you\'re just new to Goose, this is a great place to start. The best way to get good responses is to be as clear and direct as possible. Goose works best when given specific instructions, so tell it exactly what you need and include all of the important details. If you\u2019re too vague, you might end up with an overly technical or even a possibly incomplete answer that doesn\u2019t actually help you.\\n\\n\\nFor example, instead of saying:\\n\\n\u274c Okay Prompt: \\n\\n>_**Goose, what\u2019s a pull request?**_ \\n\\nThis might give you a super technical definition that assumes you already know the basics. \\n\\nSo, you could say:\\n\\n\u2705 Better Prompt:\\n>_**Goose, explain how GitHub pull requests work like I\u2019m new to coding**_ \\n\\nThis tells Goose exactly what you need and at what level. \\n\\n\\n:::tip pro tip\\nIf you want Goose to remember your preferences, you can say, \\n\\n>_**Goose, remember I\u2019m not a developer. Explain things at a high level unless I ask for technical details**_\\n\\nIf you have the [Memory Extension](/docs/tutorials/memory-mcp) enabled, Goose will save this preference so you won\u2019t have to remind it every time. \\n:::\\n\\n## Chain-of-Thought Prompting\\n\\nSometimes a topic or task can just be too much to tackle all at once, and that\u2019s where Chain-of-Thought Prompting comes in. Instead of getting this enormous and complicated response back, you can guide Goose to break things down step by step so it\u2019s easier to follow.\\n\\n\\nFor example, instead of saying:\\n\\n\u274c Okay Prompt: \\n\\n>_**Goose, what are Model Context Protocol Servers, and how are they used in goose?**_\\n\\nwhich might get you a response that\'s hard to follow, you could say:\\n\\n\u2705 Better Prompt:\\n \\n>_**Goose, walk me through what MCPs are and how they\'re used in gosoe, step by step**_ \\n\\nThis forces Goose to slow down and explain each part clearly, making it easier to understand.\\n\\nNow, if you want to take it a step further and make sure Goose understands the exact style of responses you\'re expecting, that\u2019s when Few-Shot Prompting is the way to go.\\n\\n## Few-Shot Prompting\\n\\nIf you need Goose to match a specific style or format, the best way to get there is by showing it what you want. I use this all the time! Since AI models learn patterns from examples, giving Goose a reference helps it skip the guesswork and just get straight to the point.\\n\\nFor example, instead of saying: \\n\\n\u274c Okay Prompt: \\n\\n>_**Goose, summarize this report**_ \\n\\nyou could say: \\n\\n\u2705 Better Prompt:\\n\\n>_**Goose, here\u2019s how I usually summarize reports: (example summary). Can you summarize this new report the same way?**_\\n \\nBy providing an example, you\u2019re guiding Goose to the answer that you actually want.\\n\\nNow, what if you\'ve given Goose an example and it\u2019s first response isn\u2019t quite right? There\'s no need to end the session and start over, that\u2019s when Iterative Refinement Prompting is useful.\\n\\n## Iterative Refinement Prompting\\n\\nLet\u2019s be real, Goose just like any AI agent isn\u2019t always going to get it \'right\' on the first try. Sometimes, it gives you a response that\'s way too technical, other times, it might completely miss the mark or even worse, hallucinate its way into a weird, made-up answer, that kind of sounds true. But instead of giving up and starting over, you can steer the conversation by giving feedback on what needs to change.\\n\\nSince Goose allows you to bring your own LLM, the way it responds depends a lot on which model you\u2019re using. Some LLMs need a little extra guidance, while others might require a few rounds of refinement before they get things right. The good news? You can shape the response without completely starting over.\\n\\nFor example, if Goose spits out something overly complicated, don\u2019t just accept it, you can push back! Try saying:\\n\\n>_**Goose, this response is too technical. Can you simplify it?**_ \\n\\nOr if something sounds off and you want to do a fact check:\\n\\n>_**Goose, where did you get that information? How do you know it\'s accurate?**_ \\n\\nThink of working with Goose like pair programming or collaborating with a coworker. Sometimes, you need to clarify what you want or redirect the conversation to get make sure you\'re both on the same page.\\n\\nBut what if you don\u2019t have a clear example or specific instructions to guide Goose? That\u2019s when I would use Zero-Shot Prompting.\\n\\n## Zero-Shot Prompting\\n\\nSometimes, you just want Goose to take a wild guess, get a little creative, and run with it. That\u2019s exactly what Zero-Shot Prompting is for, it lets Goose figure things out on its own, without any examples or extra guidance from you.\\n\\nFor example, you might say:\\n\\n>_**Goose, write me a project update for my team**_ \\n\\nor: \\n\\n>_**Goose, I want to build a cool prompt directory**_ \\n\\nI love using this approach when I have a rough idea but no real clear direction. It\u2019s like brainstorming but with AI, Goose will throw out ideas, suggest next steps, and sometimes even point out things I would\u2019ve never even thought of. More often than not, my original idea ends up 10x better just by letting Goose take the lead.\\n\\nNow, if you want Goose to not just come up with amazing ideas but also be funny, helpful, and maybe even a little nicer to you, that\u2019s when you need to put those manners you learned in elementary school to use.\\n\\n## Politeness-Based Prompting\\n\\nBelieve it or not, being polite actually makes AI responses better! Even though Goose isn\u2019t self-aware\u2026\u2026yet\u2026\u2026 \ud83d\udc40, AI models tend to generate more thoughtful, structured, and sometimes even friendlier replies when asked nicely. So yes, saying \u201cplease\u201d and \u201cthank you\u201d actually makes a difference.\\n\\nFor example, instead of saying:\\n\\n\u274c Okay Prompt:\\n\\n>_**Goose, generate a project update**_ \\n\\nyou could say:\\n\\n\u2705 Better Prompt:\\n\\n>_**Goose, could you generate a project update for me, please? Thanks!**_ \\n\\nGoose will respond either way, but **trust me**, polite prompts tend to get you better answers. One of our users once got the sweetest response from Goose at the end of a project, like it was genuinely grateful for the collaboration and even wished them sweet dreams. It was adorable!!\\n\\n![goose response](politenessprompt.png)\\n>_Here\u2019s the actual response, Goose is really out here making people\u2019s day._\\n\\nAnd the best part? This works with any prompting style. So, if you want Goose to be helpful, clear, and maybe even a little extra nice to you, be good to Goose and Goose will be good to you.\\n\\n## The Best Prompts Feel Natural\\n\\nAt the end of the day, all these prompting styles are just tools, at your disposal. The most important thing is to keep your prompts clear and natural. You don\u2019t have to overthink it, but adding a little structure can make a huge difference in getting the responses you actually want.\\n\\nGoose is here to make your life easier, so the next time you open up a session, just keep your goal in mind, experiment with different prompting styles, and see what works best for you.\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"AI Prompting 101: How to Get the Best Responses from Your AI Agent\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/13/better-ai-prompting\\" />\\n  <meta property=\\"og:description\\" content=\\"How to prompt and vibe code your way to better responses.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/prompt-078b12695f95c4f0eac3861a8a2611ef.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"AI Prompting 101: How to Get the Best Responses from Your AI Agent\\" />\\n  <meta name=\\"twitter:description\\" content=\\"How to prompt and vibe code your way to better responses.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/prompt-078b12695f95c4f0eac3861a8a2611ef.png\\" />\\n</head>"},{"id":"/2025/03/18/goose-langfuse","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/18/goose-langfuse","source":"@site/blog/2025-03-18-goose-langfuse/index.md","title":"How Goose Catches AI Errors with Langfuse","description":"Get detailed insights into Goose\'s behavior with Langfuse\'s observability tools.","date":"2025-03-18T00:00:00.000Z","tags":[],"readingTime":3.46,"hasTruncateMarker":true,"authors":[{"name":"Tania Chakraborty","title":"Senior Technical Community Manager","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/tania"},"socials":{"linkedin":"https://www.linkedin.com/in/taniachakraborty/","x":"https://x.com/taniashiba","github":"https://github.com/taniashiba"},"imageURL":"https://avatars.githubusercontent.com/u/126204004?v=4","key":"tania"}],"frontMatter":{"title":"How Goose Catches AI Errors with Langfuse","description":"Get detailed insights into Goose\'s behavior with Langfuse\'s observability tools.","authors":["tania"]},"unlisted":false,"prevItem":{"title":"AI Prompting 101: How to Get the Best Responses from Your AI Agent","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/19/better-ai-prompting"},"nextItem":{"title":"AI, But Make It Local With Goose and Ollama","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/14/goose-ollama"}},"content":"![blog cover](goose_aierrors.png)\\n\\nHow do we debug AI agents like Goose? In the [Goosing Around](https://youtube.com/playlist?list=PLyMFt_U2IX4uFFhd_2TD9-tlJkgHMMb6F&feature=shared) stream series, host [Rizel Scarlett](https://www.linkedin.com/in/rizel-bobb-semple/) invited [Marc Klingen](https://www.linkedin.com/in/marcklingen/), Co-Founder at Langfuse, and [Alice Hau](https://www.linkedin.com/in/alice-hau/), Machine Learning Engineer at Block, to demo how Langfuse enables observability into Goose\'s actions, letting you trace LLM behavior and catch errors.\\n\\n\x3c!--truncate--\x3e\\n\\n## What is Langfuse\\n\\n[Langfuse](https://langfuse.com/) is an open source observability platform specifically designed for LLM-powered apps. Mark revealed during stream that Langfuse wasn\'t originally an observability platform, it was born from early attempts to build an AI agent like Goose. \\n\\nWhile they were limited by the available models at the time, especially with multi-file edits, the team discovered the tooling they had built for debugging and monitoring their agent was more valuable to them than their agent.\\n\\n## How Langfuse Works With Goose\\nSince traditional observability tools don\'t quite cut it when it comes to AI agents. Langfuse introduces 3 core concepts to make Goose\'s behavior more observable, and create logs that are easier to parse:\\n\\n### Traces\\n\\nEach interaction with Goose creates a trace to capture the full story of what happened. These traces include key information, from the initial prompt and user messages to tool calls and their responses. They also store valuable metadata about model outputs and timing information, giving developers a complete picture of each interaction.\\n\\n### Timeline View\\nThe timeline view takes these complex interactions and transforms it into a digestible format. Developers can see parallel task execution in real-time, understand the dependencies between different actions, and measure the actual duration of each opersation. This can be super helpful when debugging a complex sequence of actions taken by Goose, or to help optimize performance.\\n\\n### Structured Data\\nAlice explained, \\"Goose sessions can be really long... we have log files, but you\'ll just see a massive log of JSON.\\"\\n\\nRather than rangle raw JSON log, Langfuse helps organize this data to help make navigating longer sessions and their data more straightforward. This approach can help developers easily analyze tool usage patterns, monitor token consumption, and quickly identify any performance bottlenecks and where they may happen.\\n\\nWith this integration, you can instead better understand the sequence of actions taken by Goose, and analyze track token usage and model behavior across LLMs.\\n\\n## Practical Benefits\\nThe observability the Goose and Langfuse integration brings is great for anyone who wants clear insight into what Goose is doing behind the scenes. Alice and Marc discussed the different ways this integration can help you debug faster.\\n\\nDevelopers can dive deeper into detailed session logs and identify the root cause to a reported issue and ensure Goose is operating as efficiently as possible. Like checking why certain commands may not be working as expected, or seeing exactly how Goose is processing information for a given task with a specific LLM. \\n\\nAs developers focus on operational efficiency, researchers can use the analytical capabilities of this integration to better understand which models best suit their needs. Through comprehensive model evaluations, they can analyze how different models handle tool calling, understand decision-making patterns across LLMs, and establish a systematic approach to understanding and improving AI systems.\\n\\n# The Future of AI Observability\\nThese powerful debugging and analysis capabilities are only the beginning. This integration between Goose and Langfuse represents a significant step forward in making AI agents as transparent and debuggable as traditional code.\\n\\nTo keep up with the exciting developments as they release, you can check out both of the [Goose](https://github.com/block/goose) and [Langfuse](https://github.com/langfuse/langfuse) repositories on GitHub. \\n\\nYou can also watch the [livestream discussing the Goose and Langfuse integration](https://www.youtube.com/live/W39BQjsTS9E?feature=shared), and follow the [tutorial showing you how to integrate Langfuse with Goose](https://block.github.io/goose/docs/tutorials/langfuse).\\n\\nAlso, be sure to subscribe to our [events calendar](https://calget.com/c/t7jszrie) to catch upcoming events.\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"How Goose Catches AI Errors with Langfuse\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/18/goose-langfuse\\" />\\n  <meta property=\\"og:description\\" content=\\"Get detailed insights into Goose\'s behavior with Langfuse\'s observability tools.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/goose_aierrors-22154af884db86789ce1a12a72897e8e.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"How Goose Catches AI Errors with Langfuse\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Get detailed insights into Goose\'s behavior with Langfuse\'s observability tools.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/goose_aierrors-22154af884db86789ce1a12a72897e8e.png\\" />\\n</head>"},{"id":"/2025/03/14/goose-ollama","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/14/goose-ollama","source":"@site/blog/2025-03-14-goose-ollama/index.md","title":"AI, But Make It Local With Goose and Ollama","description":"Integrate Goose with Ollama for a fully local experience.","date":"2025-03-14T00:00:00.000Z","tags":[],"readingTime":5.07,"hasTruncateMarker":true,"authors":[{"name":"Tania Chakraborty","title":"Senior Technical Community Manager","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/tania"},"socials":{"linkedin":"https://www.linkedin.com/in/taniachakraborty/","x":"https://x.com/taniashiba","github":"https://github.com/taniashiba"},"imageURL":"https://avatars.githubusercontent.com/u/126204004?v=4","key":"tania"}],"frontMatter":{"title":"AI, But Make It Local With Goose and Ollama","description":"Integrate Goose with Ollama for a fully local experience.","authors":["tania"]},"unlisted":false,"prevItem":{"title":"How Goose Catches AI Errors with Langfuse","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/18/goose-langfuse"},"nextItem":{"title":"Turn Figma Designs Into Code With Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/12/goose-figma-mcp"}},"content":"![blog cover](gooseollama.png)\\n\\nOn the [Goosing Around](https://youtube.com/playlist?list=PLyMFt_U2IX4uFFhd_2TD9-tlJkgHMMb6F&feature=shared) stream series, host [Rizel Scarlett](https://www.linkedin.com/in/rizel-bobb-semple/) [demonstrated how to use Goose locally with Ollama](https://youtube.com/watch?v=WG10r2N0IwM?feature=share) for a fully local experience on your device. Her guest, [Parth Sareen](https://www.linkedin.com/in/parthsareen/), an experienced software engineer with a focus on building frameworks and systems for AI/ML, showed us the magic of structured outputs and how Goose and Ollama work together under the hood.\\n\\n\x3c!--truncate--\x3e\\n\\nGoose serves as an on-machine AI agent that can interact with your applications and tools through extensions, providing the framework and interface for AI-powered workflows. Ollama enables running large language models locally with a simple API, handling the optimization of models to run efficiently on consumer hardware.\\n\\nTogether, they create a self-contained AI agent workflow that puts advanced capabilities directly in the hands of developers.\\n\\n# Getting Started\\n\\nBefore diving deep into various capabilities, Rizel walked us through how to set yourself up for success by integrating Goose with Ollama. To follow along, you can download Goose [here](https://block.github.io/goose/) and follow a step-by-step walk through in the [Configure LLM Provider](https://block.github.io/goose/docs/getting-started/providers) guide.\\n\\nIf you have any questions or get stuck, feel free to chat with us on [Discord](https://discord.gg/block-opensource) or post an issue/discussion on [GitHub](https://github.com/block/goose/). Thanks for reading!\\n\\n# Why Go Local?\\nUsing cloud-based LLMs and providers make it so you don\'t need substantial computing resources, so why go local? Here\'s some benefits you may want to consider:\\n\\n- **True data privacy** since your conversations never leave your device. You have complete control over sensitive information. As Parth emphasized during the discussion, \\"Your data stays with you, period.\\"\\n- **Offline capability** transforms when and where you can use AI. \\"I use Ollama all the time on planes\u2014it\'s a lot of fun!\\" Parth shared, highlighting how local models free you from the constraints of internet connectivity.\\n- **Direct control over model behavior** means you can fine-tune parameters without subscription fees or API limits. Open source models allow you to get a closer look at what\'s happening behind the scenes.\\n\\nPersonal use cases like development assistance, personal knowledge management, education, and content management are but some examples that can benefit from working locally and offline. You can keep research and sensitive data private, and utilize Goose when you have limited connectivity.\\n\\n# Can My Machine Handle This?\\nThis question came up repeatedly, and the answer is more encouraging than you think. As Parth pointed out, \\"You don\'t need to run the largest models to get excellent results.\\" The requirements you\'ll want to look out for on your device boils down to this:\\n\\n- **RAM is key**: 32GB is a solid baseline for larger models and outputs.\\n- **For MacBooks, RAM is your primary concern** given the unified memory architecture.\\n- **For Windows/Linux, GPU memory is more important** for acceleration\\n\\nUse cases can start with smaller, more efficient models that run on modest hardware. Models optimized for efficiency can deliver impressive performance even on standard laptops! Just start with a smaller model to test your workflow, then scale up as you need. This way you can figure out if you need the beefy hardware or not.\\n\\n# The Magic of Structured Outputs\\nOllama supports [structured outputs](https://ollama.com/blog/structured-outputs), making it possible to constrain a model\u2019s output to a specific format\u2014essentially teaching models to respond in specific formats like JSON. Parth explained the concept with an elegant analogy: \\"It\'s like teaching someone math operations. You show them how to add, subtract, multiply, and then they can solve different problems following those patterns.\\"\\n\\nParth showed us how these structured outputs can dramatically improve reliability. By constraining the model to respond within specific parameters, you get more consistent, predictable results. This structured approach ensures the model\'s response can be reliably parsed and integrated into applications\u2014all while running locally on your device.\\n\\nHere\'s an example of how to structure an output from the livestream:\\n\\n```json\\n// Example of image analysis with structured output\\n{\\n  \\"scene\\": \\"sunset over mountains\\",\\n  \\"objects\\": [\\n    {\\n      \\"type\\": \\"sun\\",\\n      \\"attributes\\": [\\"orange\\", \\"setting\\", \\"partially visible\\"],\\n      \\"confidence\\": 0.95\\n    },\\n    {\\n      \\"type\\": \\"mountains\\",\\n      \\"attributes\\": [\\"silhouetted\\", \\"range\\", \\"distant\\"],\\n      \\"confidence\\": 0.92\\n    },\\n    {\\n      \\"type\\": \\"sky\\",\\n      \\"attributes\\": [\\"gradient\\", \\"orange to purple\\", \\"clear\\"],\\n      \\"confidence\\": 0.98\\n    }\\n  ],\\n  \\"mood\\": \\"peaceful\\",\\n  \\"lighting\\": \\"golden hour\\",\\n  \\"composition\\": \\"rule of thirds\\"\\n}\\n```\\nAs Parth walked through these examples, he shared key practices to ensure you get the most out of local LLMs:\\n\\n1. **For precision tasks, lower the temperature**. Setting it to 0 makes responses more deterministic and factual.\\n2. **Use structured outputs whenever possible**, be explicit about the format you want in your prompts.\\n3. **Be mindful of context windows**, local models have limits on how much information they can process at once.\\n4. **Experiment with different models**! Each has strengths and weaknesses you\'ll want to explore for your needs.\\n5. **For larger documents, chunk them** into manageable pieces, this helps a lot when you\'re working with larger files.\\n\\n# It\'s About The Freedom To Choose\\nWhile there are trade-offs in terms of raw processing power when you go local vs cloud, you don\'t have to choose one over the other. As Parth summarized during the livestream: \\"Local AI isn\'t about replacing cloud options\u2014it\'s about having the freedom to choose the right approach for your specific needs.\\"\\n\\nThe benefits of owning your AI experience can be compelling for a variety of use cases. Whether you\'re a developer building tools, a writer working with confidential material, or simply someone who values privacy and control, I hope the Goose-Ollama integration offers a glimpse into how a local experience can benefit you, and explore a future where sophisticated AI is as personal and private as the data on your hard drive. Thanks for reading!\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Goosing Around: AI, But Make It Local With Goose and Ollama\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/13/goose-ollama-local\\" />\\n  <meta property=\\"og:description\\" content=\\"Integrate Goose with Ollama for a fully local experience.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/gooseollama-fbb2cb67117c81eaa189a6b6174e6c6c.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Goosing Around: AI, But Make It Local With Goose and Ollama\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Integrate Goose with Ollama for a fully local experience.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/gooseollama-fbb2cb67117c81eaa189a6b6174e6c6c.png\\" />\\n</head>"},{"id":"/2025/03/12/goose-figma-mcp","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/12/goose-figma-mcp","source":"@site/blog/2025-03-12-goose-figma-mcp/index.md","title":"Turn Figma Designs Into Code With Goose","description":"Give Goose the ability to turn Figma designs into code with the Figma extension.","date":"2025-03-12T00:00:00.000Z","tags":[],"readingTime":1.8,"hasTruncateMarker":true,"authors":[{"name":"Tania Chakraborty","title":"Senior Technical Community Manager","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/tania"},"socials":{"linkedin":"https://www.linkedin.com/in/taniachakraborty/","x":"https://x.com/taniashiba","github":"https://github.com/taniashiba"},"imageURL":"https://avatars.githubusercontent.com/u/126204004?v=4","key":"tania"}],"frontMatter":{"title":"Turn Figma Designs Into Code With Goose","description":"Give Goose the ability to turn Figma designs into code with the Figma extension.","authors":["tania"]},"unlisted":false,"prevItem":{"title":"AI, But Make It Local With Goose and Ollama","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/14/goose-ollama"},"nextItem":{"title":"Automating Phone Calls with Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/10/goose-calls-vyop"}},"content":"![blog cover](goosefigma.png)\\n\\nIn our previous episode of [Goose Flight School](https://www.youtube.com/playlist?list=PLyMFt_U2IX4s1pMaidir5P4lSfjUK6Nzm), our host [Adewale Abati](https://www.linkedin.com/in/acekyd/) showed us how to use Goose to take a Figma design and transform it into a functional Nuxt application. In this stream he covered the entire process, from initial setup to final implementation, highlighting how Goose can help developers bridge the gap between design and development.\\n\\n\x3c!--truncate--\x3e\\n\\n# How It Works\\n[Extensions](https://block.github.io/goose/docs/getting-started/using-extensions) enhance Goose\'s functionality by connecting with your existing tools and workflows. They add new features, access external data resources, and integrate with other systems. Learn how multiple extensions, including Figma and Developer, worked together seamlessly to dramatically accelerate development.\\n\\n# Live Tutorial: Goose Builds Live\\nDuring the livestream, Adewale demonstrated step-by-step how Goose handled each development phase, from creating the basic application structure to generating responsive layouts using Tailwind CSS. Adewale also highlighted how Goose addresses potential limitations as you go, showcasing the powerful balance between the Goose\'s automation and developer control.\\n\\n<iframe class=\\"aspect-ratio\\" src=\\"https://www.youtube.com/embed/_9t_N9zKwKM?si=r3e1MkrjS-f2AvkI\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nThroughout the stream, Adewale shared valuable tips to prepare your design for Goose. His key recommendations include: \\n\\n* start with a well-structured Figma design\\n* use Goose to make targeted improvements after the initial generation\\n* fine-tune specific elements as needed\\n* make sure you thoroughly test for functionality and responsiveness\\n\\n# Getting Started with Goose and Figma\\nWhether you\'re a designer wanting to rapidly turn concepts into working code or a developer curious about streamlining design implementation, you can download Goose with its built-in [Developer extension](https://block.github.io/goose/docs/getting-started/installation) and add the [Figma extension](https://block.github.io/goose/v1/extensions/). \\n\\nFor step-by-step instructions, check out the [Figma tutorial](https://block.github.io/goose/docs/tutorials/figma-mcp).\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Goose Flight School: Turn Figma Designs Into Code With Goose\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/12/goose-figma-mcp\\" />\\n  <meta property=\\"og:description\\" content=\\"Give Goose the ability to turn Figma designs into code with the Figma extension.\\" />\\n  <meta property=\\"og:image\\" content=\\"http://block.github.io/goose/assets/images/goosefigma-e6f84a734bd56cb431bb02452331a5d5.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Goose Flight School: Turn Figma Designs Into Code With Goose\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Give Goose the ability to turn Figma designs into code with the Figma extension.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"http://block.github.io/goose/assets/images/goosefigma-e6f84a734bd56cb431bb02452331a5d5.png\\" />\\n</head>"},{"id":"/2025/03/10/goose-calls-vyop","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/10/goose-calls-vyop","source":"@site/blog/2025-03-10-goose-calls-vyop/index.md","title":"Automating Phone Calls with Goose","description":"Practical tips to help you use Goose more effectively and efficiently.","date":"2025-03-10T00:00:00.000Z","tags":[],"readingTime":2.325,"hasTruncateMarker":true,"authors":[{"name":"Angie Jones","title":"Head of Developer Relations","url":"https://angiejones.tech","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/angie"},"socials":{"linkedin":"https://www.linkedin.com/in/angiejones/","github":"https://github.com/angiejones","x":"https://x.com/techgirl1908"},"imageURL":"https://avatars.githubusercontent.com/u/15972783?v=4","key":"angie"}],"frontMatter":{"title":"Automating Phone Calls with Goose","description":"Practical tips to help you use Goose more effectively and efficiently.","authors":["angie"]},"unlisted":false,"prevItem":{"title":"Turn Figma Designs Into Code With Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/12/goose-figma-mcp"},"nextItem":{"title":"6 Essential Tips for Working with Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/06/goose-tips"}},"content":"<div style={{display: \'none\'}}>\\n\\n![blog cover](goose-voyp.png)\\n\\n</div>\\n\\nIn the latest episode of [Wild Goose Case](https://www.youtube.com/playlist?list=PLyMFt_U2IX4uMW9kpE1FENQUyIgLuUnWD), hosts [Ebony Louis](https://www.linkedin.com/in/ebonylouis/) and [Ace Abati](https://www.linkedin.com/in/acekyd/) explored a fascinating new way to extend Goose\u2019s automation capabilities by integrating with [VOYP](https://voyp.app/), an AI-powered system that makes phone calls. Their guest, [Paulo Taylor](https://www.linkedin.com/in/paulotaylor/), a technology veteran with over 35 years of experience, walked through how developers can use Goose to trigger and manage phone-based interactions through VOYP.\\n\\n\x3c!--truncate--\x3e\\n\\n# Expanding Goose\u2019s Reach with AI-Powered Calls\\n\\nGoose is already known for automating tasks, but you can extend that automation beyond the screen. With the [VOYP Goose Extension](goose://extension?cmd=npx&arg=-y&arg=voyp-mcp&id=voyp&name=VOYP&description=Automated%20Phone%20Calling&env=VOYP_API_KEY%3DVOYP%20API%20key), you can automate phone calls to retrieve information, handle customer interactions, or even assist with accessibility needs.\\n\\nVOYP functions as an AI call agent, using LLMs and Text-to-Speech (TTS) technology to conduct conversations over the phone. This means you can trigger phone interactions directly from Goose sessions, enabling real-world automation beyond traditional interfaces.\\n\\n# How It Works\\n\\nUnder the hood, VOYP utilizes multiple telecom providers to optimize call costs. It supports various LLMs and TTS providers, giving users flexibility in how they configure their AI caller. The integration with Goose is made possible through the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/), which allows Goose to communicate seamlessly with VOYP and other AI-driven tools.\\n\\n# Live Demo: AI Calls in Action\\nDuring the livestream, Paulo demonstrated VOYP\u2019s capabilities with a series of engaging examples. One highlight was a playful experiment where the AI made a phone call to tell a goose-themed joke. \\n\\n<iframe class=\\"aspect-ratio\\" src=\\"https://www.youtube.com/embed/Cvf6xvz1RUc?si=KQ44y6ypZFrzbest\\" title=\\"YouTube video player\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen></iframe>\\n\\nIn [another demo](https://www.youtube.com/live/g_F1u6aqohk?t=1515), Paulo had VOYP engage in a conversation with ChatGPT\u2019s phone service about time travel, showing how fluid and adaptable the AI\u2019s responses can be. He also walked through VOYP\'s real-time conversation monitoring dashboard, which provides a transparent look at how the AI processes and responds during calls.\\n\\n# Getting Started with Goose and VOYP\\nFor those eager to experiment with [VOYP](https://github.com/paulotaylor/voyp-mcp), sign up on the [VOYP website](https://voyp.app/) to create an account and obtain an API key. While calls require credits, new users receive 20 free credits for testing. The cost per call varies by region, with U.S.-based calls being the most affordable at approximately five credits per minute. To integrate VOYP with Goose, [install the VOYP extension](goose://extension?cmd=npx&arg=-y&arg=voyp-mcp&id=voyp&name=VOYP&description=Automated%20Phone%20Calling&env=VOYP_API_KEY%3DVOYP%20API%20key).\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Wild Goose Case: Automating Phone Calls with Goose and VOYP\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/06/goose-tips\\" />\\n  <meta property=\\"og:description\\" content=\\"Give Goose the ability to make phone calls with the VOYP extension.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/goose-voyp-215f3391cfbe2132542a2be63db84999.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Wild Goose Case: Automating Phone Calls with Goose and VOYP\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Give Goose the ability to make phone calls with the VOYP extension.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/goose-voyp-215f3391cfbe2132542a2be63db84999.png\\" />\\n</head>"},{"id":"/2025/03/06/goose-tips","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/03/06/goose-tips","source":"@site/blog/2025-03-06-goose-tips/index.md","title":"6 Essential Tips for Working with Goose","description":"Practical tips to help you use Goose more effectively and efficiently.","date":"2025-03-06T00:00:00.000Z","tags":[],"readingTime":5.37,"hasTruncateMarker":true,"authors":[{"name":"Angie Jones","title":"Head of Developer Relations","url":"https://angiejones.tech","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/angie"},"socials":{"linkedin":"https://www.linkedin.com/in/angiejones/","github":"https://github.com/angiejones","x":"https://x.com/techgirl1908"},"imageURL":"https://avatars.githubusercontent.com/u/15972783?v=4","key":"angie"}],"frontMatter":{"title":"6 Essential Tips for Working with Goose","description":"Practical tips to help you use Goose more effectively and efficiently.","authors":["angie"]},"unlisted":false,"prevItem":{"title":"Automating Phone Calls with Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/10/goose-calls-vyop"},"nextItem":{"title":"Let A Team of AI Agents Do It For You","permalink":"/goose/pr-preview/pr-2176/blog/2025/02/21/gooseteam-mcp"}},"content":"![goose tips](goose-tips.png)\\n\\nWorking with AI agents can sometimes feel unpredictable. After using Goose extensively for the last few months, I\'ve compiled a few key tips that will help you get the most out of this tool. No matter your workflow, these guidelines will help you work more efficiently with Goose.\\n\\n\\n\x3c!--truncate--\x3e\\n\\n## 1. Keep Sessions Focused and Short\\n\\nOne of the most common mistakes users make is trying to accomplish too much in a single session. While it might seem efficient to keep the conversation going, longer sessions can actually hinder Goose\'s performance. \\n\\nEvery message adds to the context window, which is the amount of conversation history Goose can retain at any given time. This history is made up of tokens, the individual pieces of text (words or even parts of words) that Goose processes to generate responses. More tokens don\u2019t just increase processing time, they also contribute to LLM usage costs. And once the context window fills up, older messages get pushed out, which can lead to loss of important details or unexpected behavior.\\n\\nThink of it like keeping too many browser tabs open. Eventually, it impacts performance. Instead, start fresh sessions for distinct tasks. Don\'t worry about losing context; that\'s exactly what the [Memory extension](/docs/tutorials/memory-mcp) is for. Keeping sessions focused and concise ensures more accurate, relevant responses while also keeping your LLM costs under control.\\n\\n\\n## 2. Minimize Active Extensions\\n\\nWhen it comes to Goose extensions, less is often more. It\'s tempting to enable [every available extension](https://www.pulsemcp.com/servers) just in case (I\'m guilty of this!), but this approach can be counterproductive. Each active extension adds to the system prompt, increasing complexity and making Goose work harder to decide which tools to use.\\n\\nConsider this: if you\'re cooking in a kitchen, having every possible utensil and appliance out on the counter doesn\'t make you a better chef. It just creates clutter and confusion. The same principle applies here. \\n\\nGo ahead and install any extensions that interest you, but [keep them disabled](/docs/getting-started/using-extensions#enablingdisabling-extensions) until you need them. Start with the built-in [Developer extension](/docs/tutorials/developer-mcp) enabled, which is surprisingly powerful on its own, and only enable others when you need their specific capabilities. This leads to faster responses, lower token usage, and often more focused solutions.\\n\\n:::tip Bonus Tip\\nBefore starting a complex task, ask Goose about its current capabilities. A simple prompt like \\"Do you have tools available to work with [specific technology/service]?\\" can save time and prevent false starts. Goose can tell you whether it has the necessary tools for your task, and if not, suggest which extensions you might need to enable. This quick check ensures you have the right tools ready before diving in too deep.\\n:::\\n\\n## 3. Teach Goose with .goosehints Files\\n\\n\\nOne of Goose\'s most powerful features is its ability to understand context through [.goosehints](/docs/guides/using-goosehints) files, acting like a \\"README for AI\\". These hints can be set at both the project and global levels to guide Goose\u2019s responses.\\n\\nAt the project level, placing .goosehints files in your directory helps Goose understand your structure, conventions, and special considerations. You can even use multiple files - one at the root for overall guidance and others in specific directories for more granular instructions (e.g., frontend styling conventions).\\n\\nBeyond projects, global .goosehints files (`~/.config/goose/.goosehints`) apply across all sessions, making them perfect for things like:\\n\\n* Personal coding style preferences\\n* Favorite tools and workflows\\n* Standard testing practices\\n* Documentation conventions\\n* Git commit message formatting\\n\\n## 4. Choose the Right Mode for Your Workflow\\n\\nGoose offers [different modes](/docs/guides/goose-permissions) that determine how much autonomy it has when modifying files, using extensions, and performing automated actions. \\n\\n* \u26a1\ufe0f **Auto Mode (Default):** Goose can modify, create, and delete files, as well as use extensions, without requiring approval. Best for users who want seamless automation.\\n\\n* \u2705 **Approve Mode:** Goose asks for confirmation before making changes. With [Smart Approve](/docs/guides/goose-permissions#smart-approve) enabled, it evaluates risk levels and prompts for high-risk actions while executing safe ones automatically.\\n\\n* \ud83d\udcac **Chat Mode:** Goose operates in chat-only mode, without modifying files or using extensions. Ideal for users who want AI assistance without automation.\\n\\nIf you\u2019re new to Goose or working on a critical project, Approve Mode offers a great balance of automation and oversight. For hands-free workflows, Auto Mode keeps things moving, while Chat Mode is perfect for brainstorming and general AI assistance.\\n\\n## 5. Guide Goose with Step-by-Step Execution\\n\\nComplex tasks are best handled in stages, and Goose excels when you allow it to break problems into manageable steps. Instead of expecting an instant solution, ask Goose to generate a step-by-step plan first. Review the plan to ensure it aligns with your goals, then let Goose execute each step in sequence.\\n\\nThis structured approach not only improves accuracy but also gives you more control over the process. You can pause, adjust, or refine each step as needed, giving you more control while ensuring better results.\\n\\n## 6. Refine and Iterate for Better Responses\\n\\nGoose is powerful, but like any AI, it sometimes needs a second pass to get things right. If you don\u2019t get the response you need, try refining your prompt or asking Goose to adjust its answer.\\n\\nGood iteration techniques include:\\n\\n* Asking Goose to explain its reasoning before taking action\\n* Requesting alternative solutions to compare different approaches\\n* Asking for a step-by-step breakdown of its thought process\\n* Rewording prompts to add more detail or constraints\\n\\nFor example, instead of asking, \\"Help me debug this error,\\" try, \\"I\u2019m getting a NullPointerException in my Java method. Here\u2019s the stack trace. What could be causing it?\\" A small tweak in how you ask can dramatically improve the quality of the response.\\n\\n---\\n\\nBy following these tips, you\'ll be able to work more effectively with Goose, getting better results while using fewer resources. Remember, the goal is to solve problems efficiently and effectively. Whether you\'re writing code, automating tasks, or managing complex projects, these guidelines will help you make the most of what Goose has to offer.\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"6 Essential Tips for Working with Goose\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/03/06/goose-tips\\" />\\n  <meta property=\\"og:description\\" content=\\"Practical tips to help you use Goose more effectively and efficiently.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/goose-tips-4add28cc7201737dfb468ad11980f070.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"6 Essential Tips for Working with Goose\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Practical tips to help you use Goose more effectively and efficiently.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/goose-tips-4add28cc7201737dfb468ad11980f070.png\\" />\\n</head>"},{"id":"/2025/02/21/gooseteam-mcp","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/02/21/gooseteam-mcp","source":"@site/blog/2025-02-21-gooseteam-mcp/index.md","title":"Let A Team of AI Agents Do It For You","description":"Community Spotlight on Cliff Hall\'s GooseTeam MCP server.","date":"2025-02-21T00:00:00.000Z","tags":[],"readingTime":2.345,"hasTruncateMarker":true,"authors":[{"name":"Tania Chakraborty","title":"Senior Technical Community Manager","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/tania"},"socials":{"linkedin":"https://www.linkedin.com/in/taniachakraborty/","x":"https://x.com/taniashiba","github":"https://github.com/taniashiba"},"imageURL":"https://avatars.githubusercontent.com/u/126204004?v=4","key":"tania"}],"frontMatter":{"title":"Let A Team of AI Agents Do It For You","description":"Community Spotlight on Cliff Hall\'s GooseTeam MCP server.","authors":["tania"]},"unlisted":false,"prevItem":{"title":"6 Essential Tips for Working with Goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/03/06/goose-tips"},"nextItem":{"title":"Agentic AI and the MCP Ecosystem","permalink":"/goose/pr-preview/pr-2176/blog/2025/02/17/agentic-ai-mcp"}},"content":"![blog banner](gooseteam-mcp.png)\\n\\nDuring our [previous livestream](https://youtu.be/9tq-QUnE29U), Aaron Goldsmith, Infrastructure Operations Engineer at Cash App, showed a team of Goose AI agents collaborating in real time to create a website. Our community loved it so much, Cliff Hall was inspired to iterate on that idea and create a GooseTeam MCP server.\\n\\n\x3c!--truncate--\x3e\\n\\n## The Original Protocol\\n\\nAaron Goldsmith made an AI agent team consisting of multiple Goose instances a reality with his lightweight [Agent Communication Protocol](https://gist.github.com/AaronGoldsmith/114c439ae67e4f4c47cc33e829c82fac). With it, each Goose agent enters the chat, gets assigned a role (e.g. Project Coordinator, Researcher, Web Developer), and works on its part of a given task. The protocol specifies instructions guiding how the agents should talk and behave, allowing multiple Goose agents to collaborate. It also specifies that communication between the agents should be done via a Python-based websocket server with text/markdown . \\n\\n## GooseTeam MCP Server\\n\\nIntroducing [GooseTeam](https://github.com/cliffhall/GooseTeam), created by Software Architect and community member, Cliff Hall. GooseTeam takes Aaron\'s protocol and iterates on it into an MCP server and collaboration protocol for Goose Agents. With features like task management, message storage, and agent waiting, you can have an entire team of Goose agents work together on a task or project for you.\\n\\nA Goose agent with the Project Coordinator role will assign roles to other agents, your connected agents will send messages that can retrieved at any time, and your team of agents will connect to the same MCP server to collaborate together.\\n\\n![Goose Agents](gooseteam-agents.png)\\n\\n## A New Way to Goose\\n\\nWorking with a team of AI agents on a task is a game changer. Instead of getting confused as to how to improve your prompt engineering on your own or work across sessions manually, tools like Cliff\'s GooseTeam or Aaron\'s Agent Communication Protocol help us make sure AI agents like Goose are doing the work for us as efficiently as possible. The possibilities feel endless!\\n\\n## Get Your Contribution Featured\\nHopefully this contribution inspired you as much as it inspired our community. If you have a Goose contribution or project you\'d like to share with our community, join our [Discord](https://discord.gg/block-opensource) and share your work in the **#share-your-work** channel. You may just be featured on our livestream or get a cool prize. \ud83d\udc40 You can also star Goose on GitHub or follow us on social media so you never miss an update from us. Until next time!\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Let A Team of AI Agents Do It For You\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/02/17/gooseteam-mcp\\" />\\n  <meta property=\\"og:description\\" content=\\"Community Spotlight on Cliff Hall\'s GooseTeam MCP server.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/gooseteam-mcp-082fa2890c313519c2a1637ca979c219.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Let A Team of AI Agents Do It For You\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Community Spotlight on Cliff Hall\'s GooseTeam MCP server.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/gooseteam-mcp-082fa2890c313519c2a1637ca979c219.png\\" />\\n</head>"},{"id":"/2025/02/17/agentic-ai-mcp","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/02/17/agentic-ai-mcp","source":"@site/blog/2025-02-17-agentic-ai-mcp/index.md","title":"Agentic AI and the MCP Ecosystem","description":"A 101 introduction to AI Agents","date":"2025-02-17T00:00:00.000Z","tags":[],"readingTime":6.065,"hasTruncateMarker":true,"authors":[{"name":"Angie Jones","title":"Head of Developer Relations","url":"https://angiejones.tech","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/angie"},"socials":{"linkedin":"https://www.linkedin.com/in/angiejones/","github":"https://github.com/angiejones","x":"https://x.com/techgirl1908"},"imageURL":"https://avatars.githubusercontent.com/u/15972783?v=4","key":"angie"}],"frontMatter":{"title":"Agentic AI and the MCP Ecosystem","description":"A 101 introduction to AI Agents","authors":["angie"]},"unlisted":false,"prevItem":{"title":"Let A Team of AI Agents Do It For You","permalink":"/goose/pr-preview/pr-2176/blog/2025/02/21/gooseteam-mcp"},"nextItem":{"title":"Introducing codename goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/01/28/introducing-codename-goose"}},"content":"![blog banner](agentic-ai-with-mcp.png)\\n\\nIt seems like yesterday when we all were wowed by generative AI and specifically the chat interfaces that made interacting with large language models (LLMs) accessible to everyday people.\\n\\nAs amazing as this was, it was only the beginning. The next wave of AI is agentic, meaning AI systems that don\'t just respond to prompts but take actions, make decisions, and interact with external systems. This is accomplished via **AI agents**.\\n\\n\x3c!--truncate--\x3e\\n\\n## What are AI Agents?\\n\\nWhen you interact with chatbots that use AI, like ChatGPT, you can ask it how to do something, and it\'ll provide step-by-step instructions.\\n\\nFor example, if I ran into an error while coding, I could paste the error message into ChatGPT and ask it to help me debug. Because ChatGPT doesn\'t have access to my codebase, it would speculate on the cause of my error and give me a couple of possible solutions to try. I\'d then manually try these proposed solutions and return to inform ChatGPT of the results. We\'d continue this back and forth until the error is resolved or I give up.\\n\\nAI Agents greatly simplify this flow by talking with the LLM on my behalf and taking direct action to fix the problem.\\n\\n> _**An AI agent is a system that operates autonomously to accomplish a goal.**_\\n\\nBecause AI agents are connected to systems, they can analyze a situation, determine the next action, and execute it without much, if any, human intervention. This capability turns them from passive chatbots into automation assistants.\\n\\nBy using an AI agent, I can simply say \\"fix the error\\" and it\'ll have context about what\'s wrong and automatically fix the error for me.\\n\\n## How AI Agents Work with LLMs\\n\\nLLMs (e.g. GPT-4o, Claude 3.5 Sonnet, Gemini 2.0, etc) provide cognitive abilities to AI agents. Most AI agents will have a chat interface themselves where you type your prompt, and the agent will send this prompt to an LLM. At the same time, the agent will also inform the LLM of what **tools** it has access to.\\n\\n### Tool Calling\\n\\nTools are one of the most important aspects of agentic AI. AI agents are able to execute API calls via **tool calling**.\\n\\nLet\'s look at an example:\\n\\n1. A user sends a prompt to their AI agent: _\\"Fix the NullPointerException in my UserService.java file.\\"_\\n\\n2. The agent sends the user request and the list of its available tools to the LLM in a structured format.\\n```\\nUser Request: \\"Fix the NullPointerException in my UserService.java file.\\"\\n\\nAvailable Tools:\\n1. read_file(file_path: str) \u2192 Returns the contents of the specified file.\\n2. analyze_code(file_content: str) \u2192 Identifies potential errors and suggests fixes.\\n3. edit_file(file_path: str, modifications: dict) \u2192 Applies code changes.\\n4. run_tests() \u2192 Executes the test suite and returns results.\\n```\\n\\n3. The LLM analyzes the request and selects the appropriate tools. It determines that it needs to read the file in order to help.\\n```json\\n[\\n  {\\n    \\"tool\\": \\"read_file\\",\\n    \\"parameters\\": { \\"file_path\\": \\"UserService.java\\" }\\n  }\\n]\\n```\\n\\n4. The agent executes `read_file()` and sends the code to the LLM.\\n```java\\npublic class UserService {\\n    private Database db;\\n\\n    public void getUser(String userId) {\\n        System.out.println(db.findUserById(userId).getName());\\n    }\\n}\\n```\\n\\n5. The LLM recognizes the issue (`db` variable is not initialized) and now selects another tool to fix the issue.\\n```json\\n[\\n  {\\n    \\"tool\\": \\"edit_file\\",\\n    \\"parameters\\": {\\n      \\"file_path\\": \\"UserService.java\\",\\n      \\"modifications\\": {\\n        \\"line_number\\": 3,\\n        \\"replacement\\": \\"private Database db = new Database();\\"\\n      }\\n    }\\n  }\\n]\\n```\\n\\n6. The agent executes `edit_file()` to fix `UserService.java` and informs the LLM.\\n\\n7. The LLM determines that it\'s a good idea to run tests to confirm the change.\\n```json\\n[\\n  {\\n    \\"tool\\": \\"run_tests\\",\\n    \\"parameters\\": {}\\n  }\\n]\\n```\\n\\n8. The agent executes `run_tests()` and provides the results to the LLM.\\n```json\\n{\\n  \\"tests_passed\\": true,\\n  \\"failed_tests\\": []\\n}\\n```\\n\\n9. The LLM generates a final message that the agent then presents to the user.\\n```\\nI\u2019ve fixed the NullPointerException in UserService.java by initializing the db variable. All tests have passed. Let me know if you need further modifications!\\n```\\n\\nWithout tool calling, LLMs would only guess answers based on their training data. But by using tools, agents can directly access the data they need to make accurate decisions and take actions.\\n\\nIt\'s worth noting that not all agents are the same when it comes to tool access. Most proprietary agents are tightly scoped to a specific LLM and a predefined set of tools, as companies build agents tailored for their own applications.\\n\\nOther agents, like Goose, are more extensible, allowing users to configure it with the LLM of their choice, as well as add tools for various APIs, databases, and even [local environments like IDEs](/docs/tutorials/jetbrains-mcp). However, for agents to scale across different tools and systems without requiring custom integrations for each one, they need a standardized way to discover, call, and manage tools. This is exactly what the [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) provides.\\n\\n## MCP Ecosystem\\n\\nTraditional AI integrations require custom API calls for every system, making scaling difficult. MCP solves this by providing an open, universal protocol for agents to communicate with external systems dynamically.\\n\\nWith MCP, an agent like Goose can:\\n\\n* connect to any API without a developer writing manual integration code\\n* integrate with cloud services, dev tools, databases, and enterprise systems\\n* retrieve and store context to enhance reasoning\\n\\nAt the time of this writing, there are more than [1000 MCP servers](https://www.pulsemcp.com/servers) (systems that expose tools) that any MCP-enabled AI agent like Goose can connect to! These MCP servers act as bridges between agents and external systems, enabling access to APIs, databases, and development environments. Some were developed by the official API providers, while the vast majority were developed by community members. Because MCP is an open standard, anyone can build an MCP server for any resource. This greatly increases the possibilities of AI agents!\\n\\nFor example, let\'s say I want Goose to develop a new web app for me in my WebStorm IDE based on a Figma design and then commit the code to a new repo in GitHub. I can add the following MCP Servers as Goose extensions to give it all of these capabilities:\\n\\n* [Figma](/docs/tutorials/figma-mcp)\\n* [JetBrains](/docs/tutorials/jetbrains-mcp)\\n* [GitHub](/docs/tutorials/github-mcp)\\n\\nWith this, I can prompt my AI agent in natural language and it\'ll take care of the work:\\n\\n> _\\"Based on the figma design with file ID XYZ, build a web app in WebStorm and commit the code to a new GitHub repo named angiejones/myapp\\"_\\n\\nPretty powerful, right?! \\n\\n## Get Started with AI Agents\\nHopefully this has provided clear insight into what are AI agents, how they work, and what they can enable for you. [Goose](/docs/getting-started/installation) is free and open source and you can add as many [extensions](/docs/getting-started/using-extensions#adding-extensions) as you desire. This is a great way to get started with AI agents and see how they can automate tasks in your workflow to make you more efficient.\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Agentic AI and the MCP Ecosystem\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2025/02/17/agentic-ai-mcp\\" />\\n  <meta property=\\"og:description\\" content=\\"A 101 introduction to AI Agents\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/agentic-ai-with-mcp-1e3050cc8d8ae7a620440e871ad9f0d2.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Agentic AI and the MCP Ecosystem\\" />\\n  <meta name=\\"twitter:description\\" content=\\"A 101 introduction to AI Agents\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/agentic-ai-with-mcp-1e3050cc8d8ae7a620440e871ad9f0d2.png\\" />\\n</head>"},{"id":"/2025/01/28/introducing-codename-goose","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2025/01/28/introducing-codename-goose","source":"@site/blog/2025-01-28-introducing-codename-goose/index.md","title":"Introducing codename goose","description":"codename goose is your open source AI agent, automating engineering tasks and improving productivity.","date":"2025-01-28T00:00:00.000Z","tags":[],"readingTime":2.465,"hasTruncateMarker":true,"authors":[{"name":"Adewale Abati","title":"Staff Developer Advocate","url":"https://adewaleabati.com","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/adewale"},"socials":{"x":"https://x.com/ace_kyd","github":"https://github.com/acekyd"},"imageURL":"https://avatars.githubusercontent.com/u/4003538?v=4","key":"adewale"}],"frontMatter":{"title":"Introducing codename goose","description":"codename goose is your open source AI agent, automating engineering tasks and improving productivity.","authors":["adewale"]},"unlisted":false,"prevItem":{"title":"Agentic AI and the MCP Ecosystem","permalink":"/goose/pr-preview/pr-2176/blog/2025/02/17/agentic-ai-mcp"},"nextItem":{"title":"Resolving CI Issues with Goose: A Practical Walkthrough","permalink":"/goose/pr-preview/pr-2176/blog/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough"}},"content":"![Introducing codename goose](introducing-codename-goose.png)\\n\\nWe are thrilled to announce **codename goose**, your on-machine, open source AI agent built to automate your tasks. \\n\\nPowered by your choice of [large language models (LLMs)](/docs/getting-started/providers), a user-friendly desktop interface and CLI, and [extensions](/docs/getting-started/using-extensions) that integrate with your existing tools and applications, Goose is designed to enhance your productivity and workflow.\\n\\n\x3c!--truncate--\x3e\\n\\n\\nYou can think of Goose as an assistant that is ready to take your instructions, and do the work for you.\\n\\nWhile Goose\'s first use cases are engineering focused, the community has been exploring other non-engineering use cases for Goose as well. And it goes without saying, Goose is [open source](https://github.com/block/goose) \ud83c\udf89.\\n\\n\\n## How Goose Works\\n\\nGoose operates as an intelligent, autonomous agent capable of handling complex tasks through a well-orchestrated coordination of its core features:\\n  \\n- **Using Extensions**: [Extensions](/docs/getting-started/using-extensions) are key to Goose\u2019s adaptability, providing you the ability to connect with applications and tools that you already use. Whether it\u2019s connecting to GitHub, accessing Google Drive or integrating with JetBrains IDEs, the possibilities are extensive. Some of these extensions have been curated in the [extensions][extensions-directory] directory. Goose extensions are built on the [Model Context Protocol (MCP)](https://www.anthropic.com/news/model-context-protocol) - enabling you to build or bring your own custom integrations to Goose. \\n\\n- **LLM Providers**: Goose is compatible with a wide range of [LLM providers](/docs/getting-started/providers), allowing you to choose and integrate your preferred model. \\n\\n- **CLI and Desktop Support**: You can run Goose as a desktop app or through the command-line interface (CLI) using the same configurations across both.\\n\\n## Goose in Action\\n\\nGoose is able to handle a wide range of tasks, from simple to complex, across various engineering domains. Here are some examples of tasks that Goose has helped people with:\\n\\n- Conduct code migrations such as Ember to React, Ruby to Kotlin, Prefect-1 to Prefect-2 etc. \\n- Dive into a new project in an unfamiliar coding language\\n- Transition a code-base from field-based injection to constructor-based injection in a dependency injection framework.\\n- Conduct performance benchmarks for a build command using a build automation tool\\n- Increasing code coverage above a specific threshold\\n- Scaffolding an API for data retention\\n- Creating Datadog monitors\\n- Removing or adding feature flags etc.\\n- Generating unit tests for a feature\\n\\n## Getting Started\\n\\nYou can get started using Goose right away! Check out our [Quickstart](/docs/quickstart).\\n\\n\\n## Join the Goose Community\\n\\nExcited for upcoming features and events? Be sure to connect with us!\\n\\n- [GitHub](https://github.com/block/goose)\\n- [Discord](https://discord.gg/block-opensource)\\n- [YouTube](https://www.youtube.com/@blockopensource)\\n- [LinkedIn](https://www.linkedin.com/company/block-opensource)\\n- [X](https://x.com/blockopensource)\\n- [BlueSky](https://bsky.app/profile/opensource.block.xyz)\\n\\n\\n[extensions-directory]: https://block.github.io/goose/v1/extensions\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Introducing codename goose\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough\\" />\\n  <meta property=\\"og:description\\" content=\\"codename goose is your open source AI agent, automating engineering tasks and improving productivity.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/introducing-codename-goose-89cac25816e0ea215dd47d4b9768c8ab.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Introducing codename goose\\" />\\n  <meta name=\\"twitter:description\\" content=\\"codename goose is your open source AI agent, automating engineering tasks and improving productivity.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/introducing-codename-goose-89cac25816e0ea215dd47d4b9768c8ab.png\\" />\\n</head>"},{"id":"/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough","source":"@site/blog/2024-12-11-resolving-ci-issues-with-goose-a-practical-walkthrough/index.md","title":"Resolving CI Issues with Goose: A Practical Walkthrough","description":"Leverage Goose to simplify your CI debugging process, fetch detailed information about failed CI runs & annotations directly from GitHub, and even apply fixes directly.","date":"2024-12-11T00:00:00.000Z","tags":[],"readingTime":3.985,"hasTruncateMarker":true,"authors":[{"name":"Dalton Turner","title":"Software Engineer","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/dalton"},"socials":{"github":"https://github.com/dalton-turner"},"imageURL":"https://avatars.githubusercontent.com/u/78099245?v=4","key":"dalton"}],"frontMatter":{"draft":false,"title":"Resolving CI Issues with Goose: A Practical Walkthrough","description":"Leverage Goose to simplify your CI debugging process, fetch detailed information about failed CI runs & annotations directly from GitHub, and even apply fixes directly.","date":"2024-12-11T00:00:00.000Z","authors":["dalton"]},"unlisted":false,"prevItem":{"title":"Introducing codename goose","permalink":"/goose/pr-preview/pr-2176/blog/2025/01/28/introducing-codename-goose"},"nextItem":{"title":"Connecting AI Agents to Your Systems with MCP","permalink":"/goose/pr-preview/pr-2176/blog/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp"}},"content":"![CI](goose-github-ci.png)\\n\\nRunning into Continuous Integration (CI) failures in pull requests can be quite frustrating but they happen very often. In this post, we leverage the GitHub CLI (`gh`) using Goose to simplify your CI debugging process, fetch detailed information about failed CI runs and annotations directly from GitHub, and even apply fixes directly.\\n\\n\x3c!-- truncate --\x3e\\n\\n:::warning Goose Beta Version\\nThis post was written about a beta version of Goose and the commands and flow may have changed.\\n:::\\n\\n\\n## Getting Started\\n\\nBefore diving in, ensure you have the necessary tools set up.\\n\\n### 1. Install and Authenticate GitHub CLI (`gh`)\\n\\nYou\'ll need the [GitHub CLI](https://cli.github.com/) `gh` to enable Goose\'s access to CI check run details.  \\n\\n```bash\\nbrew install gh\\ngh auth login\\n```\\n\\nFollow the prompts to authenticate your account.\\n\\n\\n### 2. Configure Goose\\nEnsure Goose is configured and ready to interact with your repository and local tools. Specifically, you will need to configure a goose profile with the GitHub toolkit.\\n\\nUpdate your `profiles.yaml` file in `~/.config/goose` with the necessary toolkits:\\n\\n```yaml\\nmy-profile:\\n  provider: anthropic\\n  processor: claude-3-5-sonnet\\n  accelerator: claude-3-5-sonnet\\n  moderator: truncate\\n  toolkits:\\n    - name: developer\\n      requires: {}\\n    - name: github\\n      requires: {}\\n```\\nStart a session with the profile:\\n\\n```bash\\ngoose session start --profile my-profile\\n```\\nIf the profile\u2019s toolkits are not correctly configured, Goose might fail to access `gh` commands.\\n\\n## Resolving CI Failures with Goose\\nOnce everything is set up, we can now use Goose to tackle CI issues with the following steps:\\n\\n### Step 1: Fetch CI Failure Details\\nAsk Goose to retrieve information about the CI failures for a specific PR.\\n\\n**Example Request:**\\n\\n```\\n\\nCan you tell me about the CI failures on PR #11111 in the organization/repo repository? The check run ID is 123456.\\n```\\n\\nYou can access a check run ID by clicking on a PR\u2019s checks tab and selecting a failure. The ID will be available in the URL.\\n\\n**Goose\'s Workflow:**\\n\\nGoose will use the `gh` CLI to check run information and provide a detailed summary of the issues. Goose uses the GitHub CLI to:\\n\\n- Fetch details about failed CI runs, including logs and annotations.\\n- Provide a clear and actionable summary of issues.\\n\\n**Example Command Goose Runs:**\\n\\n```curl\\ngh api -H \\"Accept: application/vnd.github+json\\" /repos/<owner>/<repo>/check-runs/<check-run-id>/annotations\\n```\\n\\n**Example Goose Output:**\\n```mathematica\\nHere are the details for Check Run ID 123456:\\n\\n**Name**: CI-Build-All  \\n**Status**: completed  \\n**Conclusion**: failure  \\n\\n**Annotations Summary:**  \\n1. **ModuleA Test Failure**  \\n   File: `ModuleATests`  \\n   Line: 35  \\n   Error: Mismatched parameter order in function call.  \\n\\n2. **ModuleB Test Failure**  \\n   File: `ModuleBTests`  \\n   Line: 64  \\n   Error: Unexpected nil value encountered during initialization.  \\n\\n3. **ModuleC Test Failure**  \\n   File: `ModuleCTests`  \\n   Line: 98  \\n   Error: Type mismatch in returned value. Expected \'String\', found \'Int\'.  \\n```\\n### Step 2: Request Fixes from Goose\\nOnce Goose provides the failure details, ask it to resolve the issues directly.\\n\\nExample Request:\\n\\n```\\nCan you fix these issues and stage the changes so I can review them? Do not add unnecessary code comments.\\n```\\n\\n**What Goose Does:**\\n\\n- Applies fixes based on error messages.\\n- Stages changes for review.\\n\\n\\n**Example Goose Response:**\\n```markdown\\nI have fixed the following files and staged them:\\n\\n- `ModuleATests`: Corrected the order of parameters in the function call.  \\n- `ModuleBTests`: Resolved the unexpected nil value encountered during initialization.  \\n- `ModuleCTests`: Adjusted the type mismatch in the returned value.  \\n\\nYou can now review the staged changes.\\n```\\n\\n\\n### Step 3: Review the Changes\\nCarefully review the staged changes to ensure they address the issues correctly and avoid unintended modifications.\\n\\n**Things to Check:**\\n\\n- No unnecessary comments (e.g., // Fix xyz).\\n- No alterations to unrelated parts of the code.\\n\\n\\nIf needed, clean up or re-stage the changes before committing.\\n\\n## Benefits of Using Goose\\nUsing Goose to resolve CI issues streamlines your workflow by:\\n\\n- identifying issues and applying fixes with minimal manual effort.\\n- integrating with tools like the GitHub CLI to validate changes.\\n- handling repetitive CI debugging tasks while you focus on code quality.\\n\\nGoose allows you to resolve CI failures efficiently, ensuring confidence in your workflow while reducing the effort required for debugging and testing.\\n\\nTry it out, and let Goose handle the heavy lifting of CI debugging for you!\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Resolving CI Issues with Goose: A Practical Walkthrough\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough\\" />\\n  <meta property=\\"og:description\\" content=\\"Leverage Goose to simplify your CI debugging process, fetch detailed information about failed CI runs & annotations directly from GitHub, and even apply fixes directly.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/goose-github-ci-30930008ab57b0aebae15a03c73a12b5.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Resolving CI Issues with Goose: A Practical Walkthrough\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Leverage Goose to simplify your CI debugging process, fetch detailed information about failed CI runs & annotations directly from GitHub, and even apply fixes directly.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/goose-github-ci-30930008ab57b0aebae15a03c73a12b5.png\\" />\\n</head>"},{"id":"/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp","source":"@site/blog/2024-12-10-connecting-ai-agents-to-your-systems-with-mcp/index.md","title":"Connecting AI Agents to Your Systems with MCP","description":"mcp","date":"2024-12-10T00:00:00.000Z","tags":[],"readingTime":3.315,"hasTruncateMarker":true,"authors":[{"name":"Angie Jones","title":"Head of Developer Relations","url":"https://angiejones.tech","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/angie"},"socials":{"linkedin":"https://www.linkedin.com/in/angiejones/","github":"https://github.com/angiejones","x":"https://x.com/techgirl1908"},"imageURL":"https://avatars.githubusercontent.com/u/15972783?v=4","key":"angie"}],"frontMatter":{"draft":false,"title":"Connecting AI Agents to Your Systems with MCP","date":"2024-12-10T00:00:00.000Z","authors":["angie"]},"unlisted":false,"prevItem":{"title":"Resolving CI Issues with Goose: A Practical Walkthrough","permalink":"/goose/pr-preview/pr-2176/blog/2024/12/11/resolving-ci-issues-with-goose-a-practical-walkthrough"},"nextItem":{"title":"Previewing Goose v1.0 Beta","permalink":"/goose/pr-preview/pr-2176/blog/2024/12/06/previewing-goose-v10-beta"}},"content":"![mcp](goose-mcp.png)\\n\\nOpen standards are a critical ingredient for interoperable systems. They have enabled most of the technologies that we all rely on. The ability to connect to the internet no matter where we are relies on open standards such as Wi-Fi, TCP/IP and DNS. When you receive an email in your Gmail account from an Outlook sender, it\'s the use of open standards like SMTP, IMAP, and POP3 that makes this seamless. One of the most transformative technologies of our lifetime - the internet - enables anyone to have their web page accessible to the entire world thanks to the HTTP and HTML standards.\\n\\nWe\'re in the early days of a new era in tech, one where companies are innovating and building practical AI solutions for the masses. To ensure the longevity of this technology, open standards will be essential in guiding the development of AI tools so that the diverse systems built by various companies can work together seamlessly.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n### The MCP Open Standard\\n\\nAnthropic is leading the charge with the [Model Context Protocol (MCP)](https://modelcontextprotocol.io), an open standard that enables large language model (LLM) applications to connect with external systems, providing the necessary context for more informed and relevant AI interactions. \\n\\nThis is a game changer for AI agents such as [Goose](https://block.github.io/goose/), which can perform tasks autonomously - a significant leap beyond chatbots that only provide step-by-step instructions. However, to unlock the full potential of these AI agents, we need a standard method for connecting them to external data sources. MCP provides this foundation.\\n\\nWith MCP\'s standardized APIs and endpoints, Goose can integrate seamlessly into your systems, enhancing its ability to perform complex tasks like debugging, writing code, and running commands directly in your environment. \\n\\n![Goose Framework](goose-framework-1.0.png)\\n\\n### What\'s Possible\\n\\nWithout MCP, every [Goose toolkit](https://block.github.io/goose/plugins/using-toolkits.html) developer would need to implement bespoke integrations with every system they need to connect to. Not only is this tedious and repetitive, but it delays the fun stuff.\\n\\nLet\'s take a simple GitHub workflow, for example. Goose interacts directly with the GitHub API using custom scripts or configurations. Developers must configure Goose to authenticate with GitHub and specify endpoints for actions like fetching open pull requests or adding comments. Each integration requires manual setup and custom coding to handle authentication tokens, error handling, and API updates.\\n\\nMCP simplifies the process by providing a standardized interface for accessing GitHub as a resource. Goose, acting as an [MCP client](https://modelcontextprotocol.io/clients), requests the necessary information (e.g., list of open pull requests) from an [MCP server](https://modelcontextprotocol.io/quickstart#general-architecture) configured to expose GitHub\'s capabilities. The MCP server handles authentication and communication with GitHub, abstracting away the complexity of API interactions. Goose can then focus on tasks like providing a detailed review comment or suggesting code changes.\\n\\n### Join the Ecosystem\\n\\nAs MCP adoption expands, so does Goose\u2019s potential to deliver even more powerful solutions for your organization. By [integrating Goose](https://block.github.io/goose/) into your workflows and [embracing MCP](https://modelcontextprotocol.io/introduction), you\u2019re not just enhancing your own systems, you\u2019re contributing to the growth of an ecosystem that makes AI tools more interoperable, efficient, and impactful.\\n\\n\\n\\n<head>\\n  <meta charset=\\"UTF-8\\" />\\n  <title>Connecting AI Agents to Your Systems with MCP</title>\\n  <meta name=\\"description\\" content=\\"Goose\\" />\\n  <meta name=\\"keywords\\" content=\\"MCP, Anthropic, AI Open Standards\\" />\\n\\n\\n  \x3c!-- HTML Meta Tags --\x3e\\n  <title>Connecting AI Agents to Your Systems with MCP</title>\\n  <meta name=\\"description\\" content=\\"Learn how MCP standardizes integrations and fosters an ecosystem for the future of AI-enabled tools.\\" />\\n\\n  \x3c!-- Facebook Meta Tags --\x3e\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp\\" />\\n  <meta property=\\"og:type\\" content=\\"website\\" />\\n  <meta property=\\"og:title\\" content=\\"Connecting AI Agents to Your Systems with MCP\\" />\\n  <meta property=\\"og:description\\" content=\\"Learn how MCP standardizes integrations and fosters an ecosystem for the future of AI-enabled tools.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/goose-mcp-34a5252d18d18dff26157d673f7af779.png\\" />\\n\\n  \x3c!-- Twitter Meta Tags --\x3e\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io\\" />\\n  <meta property=\\"twitter:url\\" content=\\"https://block.github.io/goose/blog/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Connecting AI Agents to Your Systems with MCP\\" />\\n  <meta name=\\"twitter:description\\" content=\\"Learn how MCP standardizes integrations and fosters an ecosystem for the future of AI-enabled tools.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/goose-mcp-34a5252d18d18dff26157d673f7af779.png\\" />\\n</head>"},{"id":"/2024/12/06/previewing-goose-v10-beta","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2024/12/06/previewing-goose-v10-beta","source":"@site/blog/2024-12-06-previewing-goose-v10-beta/index.md","title":"Previewing Goose v1.0 Beta","description":"Goose v1.0 Beta is here! Learn about the latest features and improvements.","date":"2024-12-06T00:00:00.000Z","tags":[],"readingTime":2.075,"hasTruncateMarker":true,"authors":[{"name":"Adewale Abati","title":"Staff Developer Advocate","url":"https://adewaleabati.com","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/adewale"},"socials":{"x":"https://x.com/ace_kyd","github":"https://github.com/acekyd"},"imageURL":"https://avatars.githubusercontent.com/u/4003538?v=4","key":"adewale"}],"frontMatter":{"draft":false,"title":"Previewing Goose v1.0 Beta","description":"Goose v1.0 Beta is here! Learn about the latest features and improvements.","date":"2024-12-06T00:00:00.000Z","authors":["adewale"]},"unlisted":false,"prevItem":{"title":"Connecting AI Agents to Your Systems with MCP","permalink":"/goose/pr-preview/pr-2176/blog/2024/12/10/connecting-ai-agents-to-your-systems-with-mcp"},"nextItem":{"title":"Screenshot-Driven Development","permalink":"/goose/pr-preview/pr-2176/blog/2024/11/22/screenshot-driven-development"}},"content":"![Goose v1.0 Beta](goose-v1.0-beta.png)\\nWe are excited to share a preview of the new updates coming to Goose with Goose v1.0 Beta!\\n\\nThis major update comes with a bunch of new features and improvements that make Goose more powerful and user-friendly. Here are some of the key highlights.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Exciting Features of Goose 1.0 Beta\\n\\n### 1. Transition to Rust\\n\\nThe core of Goose has been rewritten in Rust. Why does this matter? Rust allows for a more portable and stable experience. This change means that Goose can run smoothly on different systems without the need for Python to be installed, making it easier for anyone to start using it.\\n\\n### 2. Contextual Memory\\n\\nGoose will remember previous interactions to better understand ongoing projects. This means you won\u2019t have to keep repeating yourself. Imagine having a conversation with someone who remembers every detail\u2014this is the kind of support Goose aims to offer.\\n\\n### 3. Improved Plugin System\\n\\nIn Goose v1.0, the Goose toolkit system is being replaced with Extensions. Extensions are modular daemons that Goose can interact with dynamically. As a result, Goose will be able to support more complex plugins and integrations. This will make it easier to extend Goose with new features and functionality.\\n\\n### 4. Headless mode\\n\\nYou can now run Goose in headless mode - this is useful for running Goose on servers or in environments where a graphical interface is not available.\\n\\n```sh\\ncargo run --bin goose -- run -i instructions.md\\n```\\n\\n### 5. Goose now has a GUI\\n\\nGoose now has an electron-based GUI macOS application that provides and alternative to the CLI to interact with Goose and manage your projects.\\n\\n![Goose GUI](goose-gui.png)\\n\\n### 6. Goose alignment with open protocols\\n\\nGoose v1.0 Beta now uses a custom protocol, that is designed in parallel with [Anthropic\u2019s Model Context Protocol](https://www.anthropic.com/news/model-context-protocol) (MCP) to communicate with Systems. This makes it possible for developers to create their own systems (e.g Jira, ) that Goose can integrate with. \\n\\nExcited for many more feature updates and improvements? Stay tuned for more updates on Goose! Check out the [Goose repo](https://github.com/block/goose) and join our [Discord community](https://discord.gg/block-opensource).\\n\\n\\n<head>\\n  <meta property=\\"og:title\\" content=\\"Previewing Goose v1.0 Beta\\" />\\n  <meta property=\\"og:type\\" content=\\"article\\" />\\n  <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2024/12/06/previewing-goose-v10-beta\\" />\\n  <meta property=\\"og:description\\" content=\\"AI Agent uses screenshots to assist in styling.\\" />\\n  <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/goose-v1.0-beta-5d469fa73edea37cfccfe8a8ca0b47e2.png\\" />\\n  <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n  <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n  <meta name=\\"twitter:title\\" content=\\"Screenshot-Driven Development\\" />\\n  <meta name=\\"twitter:description\\" content=\\"AI Agent uses screenshots to assist in styling.\\" />\\n  <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/goose-v1.0-beta-5d469fa73edea37cfccfe8a8ca0b47e2.png\\" />\\n</head>"},{"id":"/2024/11/22/screenshot-driven-development","metadata":{"permalink":"/goose/pr-preview/pr-2176/blog/2024/11/22/screenshot-driven-development","source":"@site/blog/2024-11-22-screenshot-driven-development/index.md","title":"Screenshot-Driven Development","description":"AI Agent uses screenshots to assist in styling.","date":"2024-11-22T00:00:00.000Z","tags":[],"readingTime":4.485,"hasTruncateMarker":true,"authors":[{"name":"Rizel Scarlett","title":"Staff Developer Advocate","page":{"permalink":"/goose/pr-preview/pr-2176/blog/authors/rizel"},"socials":{"x":"https://x.com/blackgirlbytes","github":"https://github.com/blackgirlbytes"},"imageURL":"https://avatars.githubusercontent.com/u/22990146?v=4","key":"rizel"}],"frontMatter":{"draft":false,"title":"Screenshot-Driven Development","description":"AI Agent uses screenshots to assist in styling.","date":"2024-11-22T00:00:00.000Z","authors":["rizel"]},"unlisted":false,"prevItem":{"title":"Previewing Goose v1.0 Beta","permalink":"/goose/pr-preview/pr-2176/blog/2024/12/06/previewing-goose-v10-beta"}},"content":"![calendar](screenshot-driven-development.png)\\n\\nI\'m a developer at heart, so when I\'m working on a personal project, the hardest part isn\'t writing code\u2014it\'s making design decisions. I recently built a calendar user interface. I wanted to enhance its visual appeal, so I researched UI design trends like \\"glassmorphism\\" and \\"claymorphism.\\"\\n\\nHowever, I didn\'t want to spend hours implementing the CSS for each design trend, so I developed a faster approach: screenshot-driven development. I used an open source developer agent called [Goose](https://github.com/block/goose) to transform my user interfaces quickly.\\n\\n\x3c!-- truncate --\x3e\\n\\n:::warning Goose Beta Version\\nThis post was written about a beta version of Goose and the commands and flow may have changed.\\n:::\\n\\n### My original calendar:\\n![calendar](screenshot-calendar-og.png)\\n\\n### Goose prototyped the designs below: \\n![Goose prototypes](goose-prototypes-calendar.png)\\n\\nIn this blog post, I\'ll show you how to quickly prototype design styles by letting Goose handle the CSS for you.\\n>\ud83d\udca1 Note: Your results might look different from my examples - that\'s part of the fun of generative AI! Each run can produce unique variations of these design trends.\\n\\n## Get Started with Screenshot-Driven Development\\n\\n### Step 1: Create your UI\\nLet\'s create a basic UI to experiment with. Create an index.html file with the code below:\\n\\n<details>\\n<summary>Create an index.html file with the code below</summary>\\n\\n```html\\n<!DOCTYPE html>\\n<html>\\n<head>\\n    <style>\\n        body {\\n            display: flex;\\n            justify-content: center;\\n            align-items: center;\\n            min-height: 100vh;\\n            margin: 0;\\n            background: linear-gradient(45deg, #6e48aa, #9c27b0);\\n            font-family: -apple-system, BlinkMacSystemFont, \\"Segoe UI\\", Roboto, sans-serif;\\n        }\\n\\n        .calendar {\\n            background: white;\\n            border-radius: 12px;\\n            box-shadow: 0 5px 20px rgba(0,0,0,0.1);\\n            width: 400px;\\n            padding: 20px;\\n        }\\n\\n        .header {\\n            display: flex;\\n            justify-content: space-between;\\n            align-items: center;\\n            padding-bottom: 20px;\\n            border-bottom: 2px solid #f0f0f0;\\n        }\\n\\n        .month {\\n            font-size: 24px;\\n            font-weight: 600;\\n            color: #1a1a1a;\\n        }\\n\\n        .days {\\n            display: grid;\\n            grid-template-columns: repeat(7, 1fr);\\n            gap: 10px;\\n            margin-top: 20px;\\n            text-align: center;\\n        }\\n\\n        .days-header {\\n            display: grid;\\n            grid-template-columns: repeat(7, 1fr);\\n            gap: 10px;\\n            margin-top: 20px;\\n            text-align: center;\\n        }\\n\\n        .days-header span {\\n            color: #666;\\n            font-weight: 500;\\n            font-size: 14px;\\n        }\\n\\n        .day {\\n            aspect-ratio: 1;\\n            display: flex;\\n            align-items: center;\\n            justify-content: center;\\n            border-radius: 50%;\\n            font-size: 14px;\\n            color: #333;\\n            cursor: pointer;\\n            transition: all 0.2s;\\n        }\\n\\n        .day:hover {\\n            background: #f0f0f0;\\n        }\\n\\n        .day.today {\\n            background: #9c27b0;\\n            color: white;\\n        }\\n\\n        .day.inactive {\\n            color: #ccc;\\n        }\\n    </style>\\n</head>\\n<body>\\n    <div class=\\"calendar\\">\\n        <div class=\\"header\\">\\n            <div class=\\"month\\">November 2024</div>\\n        </div>\\n        <div class=\\"days-header\\">\\n            <span>Sun</span>\\n            <span>Mon</span>\\n            <span>Tue</span>\\n            <span>Wed</span>\\n            <span>Thu</span>\\n            <span>Fri</span>\\n            <span>Sat</span>\\n        </div>\\n        <div class=\\"days\\">\\n            <div class=\\"day inactive\\">27</div>\\n            <div class=\\"day inactive\\">28</div>\\n            <div class=\\"day inactive\\">29</div>\\n            <div class=\\"day inactive\\">30</div>\\n            <div class=\\"day inactive\\">31</div>\\n            <div class=\\"day\\">1</div>\\n            <div class=\\"day\\">2</div>\\n            <div class=\\"day\\">3</div>\\n            <div class=\\"day\\">4</div>\\n            <div class=\\"day\\">5</div>\\n            <div class=\\"day\\">6</div>\\n            <div class=\\"day\\">7</div>\\n            <div class=\\"day\\">8</div>\\n            <div class=\\"day\\">9</div>\\n            <div class=\\"day\\">10</div>\\n            <div class=\\"day\\">11</div>\\n            <div class=\\"day\\">12</div>\\n            <div class=\\"day\\">13</div>\\n            <div class=\\"day today\\">14</div>\\n            <div class=\\"day\\">15</div>\\n            <div class=\\"day\\">16</div>\\n            <div class=\\"day\\">17</div>\\n            <div class=\\"day\\">18</div>\\n            <div class=\\"day\\">19</div>\\n            <div class=\\"day\\">20</div>\\n            <div class=\\"day\\">21</div>\\n            <div class=\\"day\\">22</div>\\n            <div class=\\"day\\">23</div>\\n            <div class=\\"day\\">24</div>\\n            <div class=\\"day\\">25</div>\\n            <div class=\\"day\\">26</div>\\n            <div class=\\"day\\">27</div>\\n            <div class=\\"day\\">28</div>\\n            <div class=\\"day\\">29</div>\\n            <div class=\\"day\\">30</div>\\n        </div>\\n    </div>\\n</body>\\n</html>\\n```\\n</details>\\n\\nOnce saved, open the file in your browser. You should see a calendar!\\n\\n### Step 2: Install Goose\\n\\n```bash\\nbrew install pipx\\npipx ensurepath\\npipx install goose-ai\\n```\\n\\n### Step 3: Start a session\\n\\n```bash\\ngoose session start\\n```\\n\\n#### Bring your own LLM\\n\\n>Goose will prompt you to set up your API key when you first run this command. You can use various LLM providers like OpenAI or Anthropic\\n\\n```bash\\nexport OPENAI_API_KEY=your_api_key\\n# Or for other providers:\\nexport ANTHROPIC_API_KEY=your_api_key\\n```\\n\\n### Step 4: Enable the Screen toolkit\\nGoose uses [toolkits](https://block.github.io/goose/plugins/plugins.html) to extend its capabilities. The [screen](https://block.github.io/goose/plugins/available-toolkits.html#6-screen-toolkit) toolkit lets Goose take and analyze screenshots.\\n\\nTo enable the Screen toolkit, add it to your Goose profile at ~/.config/goose/profiles.yaml.\\n\\n> Your configuration might look slightly different depending on your LLM provider preferences.\\n\\n\\n```yaml\\ndefault:\\n  provider: openai\\n  processor: gpt-4o\\n  accelerator: gpt-4o-mini\\n  moderator: truncate\\n  toolkits:\\n  - name: developer\\n    requires: {}\\n  - name: screen\\n    requires: {}\\n```\\n\\n### Step 5: Prompt Goose to screenshot your UI\\nGoose analyzes your UI through screenshots to understand its structure and elements. In your Gooses session, prompt Goose to take a screenshot by specifying which display your UI is on:\\n\\n```bash\\nTake a screenshot of display(1)  \\n```\\n\\n> The display number is required - use display(1) for your main monitor or display(2) for a secondary monitor.\\n\\nUpon success, Goose will run a `screencapture` command and save it as a temporary file.\\n\\n### Step 6: Prompt Goose to transform your UI\\n\\nNow, you can ask Goose to apply different design styles. Here are some of the prompts I gave Goose and the results it produced:\\n\\n#### Glassmorphism\\n\\n```bash\\nApply a glassmorphic effect to my UI\\n```\\n\\n![glassmorphism](glassmorphism-calendar.png)\\n\\n\\n#### Neumorphism\\n\\n```bash\\nApply neumorphic effects to my calendar and the dates\\n```\\n\\n![neumorphism](neumorphism-calendar.png)\\n\\n\\n#### Claymorphism\\n\\n```bash\\nPlease replace with a claymorphic effect\\n```\\n\\n![claymorphism](claymorphism-calendar.png)\\n\\n\\n#### Brutalism\\n\\n```bash\\nApply a brutalist effect please\\n```\\n\\n![brutalism](brutalism-calendar.png)\\n\\n## Learn More\\n\\nDeveloping user interfaces is a blend of creativity and problem-solving. And I love that using Goose gives me more time to focus on creativity rather than wrestling with CSS for hours. \\n\\nBeyond prototyping, Goose\'s ability to analyze screenshots can help developers identify and resolve UI bugs.\\n\\nIf you\'re interested in learning more, check out the [Goose repo](https://github.com/block/goose) and join our [Discord community](https://discord.gg/block-opensource).\\n\\n<head>\\n    <meta property=\\"og:title\\" content=\\"Screenshot-Driven Development\\" />\\n    <meta property=\\"og:type\\" content=\\"article\\" />\\n    <meta property=\\"og:url\\" content=\\"https://block.github.io/goose/blog/2024/11/22/screenshot-driven-development\\" />\\n    <meta property=\\"og:description\\" content=\\"AI Agent uses screenshots to assist in styling.\\" />\\n    <meta property=\\"og:image\\" content=\\"https://block.github.io/goose/assets/images/screenshot-driven-development-4ed1beaa10c6062c0bf87e2d27590ad6.png\\" />\\n    <meta name=\\"twitter:card\\" content=\\"summary_large_image\\" />\\n    <meta property=\\"twitter:domain\\" content=\\"block.github.io/goose\\" />\\n    <meta name=\\"twitter:title\\" content=\\"Screenshot-Driven Development\\" />\\n    <meta name=\\"twitter:description\\" content=\\"AI Agent uses screenshots to assist in styling.\\" />\\n    <meta name=\\"twitter:image\\" content=\\"https://block.github.io/goose/assets/images/screenshot-driven-development-4ed1beaa10c6062c0bf87e2d27590ad6.png\\" />\\n</head>"}]}}')}}]);